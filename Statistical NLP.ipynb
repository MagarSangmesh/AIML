{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af92136-496b-4c1b-aa60-90a7d33ff561",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4ebbb-2b04-4d22-a448-ab192ba1ba9e",
   "metadata": {},
   "source": [
    "__DOMAIN__: Digital content management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8828e-d830-4c50-9ec7-e13365b82a1a",
   "metadata": {},
   "source": [
    "__CONTEXT__: Classification is probably the most popular task that you would deal with in real life. Text in the form of blogs, posts, articles, etc. is written every second. It is a challenge to predict the information about the writer without knowing about him/her. We are going to create a classifier that predicts multiple features of the author of a given text. We have designed it as a Multi label classi ication problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af589b18-c55b-481b-a720-c4413bc2096f",
   "metadata": {},
   "source": [
    "__DATA DESCRIPTION__: Over 600,000 posts from more than 19 thousand bloggers The Blog Authorship Corpus consists of the collected\n",
    "posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person. Each blog is presented as a separate file, the name of which indicates a blogger `id#` and the blogger’s self-provided `gender`, `age`, `industry`, and `astrological sign`. (All are labelled for gender and age but for many, industry and/or sign is marked as unknown.) All bloggers included in the corpus fall into one of three age groups:\n",
    "\n",
    "• 8240 \"10s\" blogs (ages 13-17),\n",
    "\n",
    "• 8086 \"20s\" blogs(ages 23-27) and\n",
    "\n",
    "• 2994 \"30s\" blogs (ages 33-47)\n",
    "\n",
    "For each age group, there is an equal number of male and female bloggers. Each blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label url link. Link to dataset: https://www.kaggle.com/rtatman/blog-authorship-corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2086e83e-c130-4bec-9307-383b5e8cd9ba",
   "metadata": {},
   "source": [
    "__PROJECT OBJECTIVE__: The need is to build a NLP classifier which can use input text parameters to determine the label/s of the blog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b5a95-3a3e-447e-bc41-12a2530091b7",
   "metadata": {},
   "source": [
    "### Steps and tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15cd42b-ed5d-43c0-b311-c4a8de5ec4fc",
   "metadata": {},
   "source": [
    "#### 1. Read and Analyse Dataset. [5 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56a7ee-58ed-4456-b7be-423f15152e78",
   "metadata": {},
   "source": [
    "__A. Clearly write outcome of data analysis(Minimum 2 points) [2 Marks]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b9c115-0bd0-4f5d-9a94-c2e20afe6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import gc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from langdetect import detect\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, make_scorer, f1_score\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b496bb89-4bc5-4279-9d00-87266cf75451",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"blogtext.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaab3b81-9c25-4e42-8796-bf3464d8dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>I had an interesting conversation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Somehow Coca-Cola has a way of su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>If anything, Korea is a country o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Take a read of this news article ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>I surf the English news sites a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>Ah, the Korean language...it look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>If you click on my profile you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>Last night was pretty fun...mostl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>There is so much that is differen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>urlLink    Here it is, the super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>One thing I love about Seoul (and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>urlLink    Wonderful oh-gyup-sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>18,June,2004</td>\n",
       "      <td>Here is the latest from the Korea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>17,June,2004</td>\n",
       "      <td>Well, I stand corrected, again.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>16,June,2004</td>\n",
       "      <td>So I've been in Vancouver a few d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id gender  age              topic      sign          date  \\\n",
       "0   2059027   male   15            Student       Leo   14,May,2004   \n",
       "1   2059027   male   15            Student       Leo   13,May,2004   \n",
       "2   2059027   male   15            Student       Leo   12,May,2004   \n",
       "3   2059027   male   15            Student       Leo   12,May,2004   \n",
       "4   3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "5   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "6   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "7   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "8   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "9   3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "10  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "11  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "12  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "13  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "14  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "15  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "16  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "17  3581210   male   33  InvestmentBanking  Aquarius  18,June,2004   \n",
       "18  3581210   male   33  InvestmentBanking  Aquarius  17,June,2004   \n",
       "19  3581210   male   33  InvestmentBanking  Aquarius  16,June,2004   \n",
       "\n",
       "                                                 text  \n",
       "0              Info has been found (+/- 100 pages,...  \n",
       "1              These are the team members:   Drewe...  \n",
       "2              In het kader van kernfusie op aarde...  \n",
       "3                    testing!!!  testing!!!            \n",
       "4                Thanks to Yahoo!'s Toolbar I can ...  \n",
       "5                I had an interesting conversation...  \n",
       "6                Somehow Coca-Cola has a way of su...  \n",
       "7                If anything, Korea is a country o...  \n",
       "8                Take a read of this news article ...  \n",
       "9                I surf the English news sites a l...  \n",
       "10               Ah, the Korean language...it look...  \n",
       "11               If you click on my profile you'll...  \n",
       "12               Last night was pretty fun...mostl...  \n",
       "13               There is so much that is differen...  \n",
       "14                urlLink    Here it is, the super...  \n",
       "15               One thing I love about Seoul (and...  \n",
       "16                urlLink    Wonderful oh-gyup-sal...  \n",
       "17               Here is the latest from the Korea...  \n",
       "18               Well, I stand corrected, again.  ...  \n",
       "19               So I've been in Vancouver a few d...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcfe8bf6-6eb7-45c6-a730-c7a1201d92f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 681284 entries, 0 to 681283\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   id      681284 non-null  int64 \n",
      " 1   gender  681284 non-null  object\n",
      " 2   age     681284 non-null  int64 \n",
      " 3   topic   681284 non-null  object\n",
      " 4   sign    681284 non-null  object\n",
      " 5   date    681284 non-null  object\n",
      " 6   text    681284 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 36.4+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5fc81ce-7042-40df-a407-7aad46dc888b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.812840e+05</td>\n",
       "      <td>681284</td>\n",
       "      <td>681284.000000</td>\n",
       "      <td>681284</td>\n",
       "      <td>681284</td>\n",
       "      <td>681284</td>\n",
       "      <td>681284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>2616</td>\n",
       "      <td>611652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>urlLink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>345193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>251015</td>\n",
       "      <td>65048</td>\n",
       "      <td>16544</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.397802e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.932326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.247723e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.786009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.114000e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.239610e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.607577e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.525660e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.337650e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  gender            age   topic    sign            date  \\\n",
       "count   6.812840e+05  681284  681284.000000  681284  681284          681284   \n",
       "unique           NaN       2            NaN      40      12            2616   \n",
       "top              NaN    male            NaN  indUnk  Cancer  02,August,2004   \n",
       "freq             NaN  345193            NaN  251015   65048           16544   \n",
       "mean    2.397802e+06     NaN      23.932326     NaN     NaN             NaN   \n",
       "std     1.247723e+06     NaN       7.786009     NaN     NaN             NaN   \n",
       "min     5.114000e+03     NaN      13.000000     NaN     NaN             NaN   \n",
       "25%     1.239610e+06     NaN      17.000000     NaN     NaN             NaN   \n",
       "50%     2.607577e+06     NaN      24.000000     NaN     NaN             NaN   \n",
       "75%     3.525660e+06     NaN      26.000000     NaN     NaN             NaN   \n",
       "max     4.337650e+06     NaN      48.000000     NaN     NaN             NaN   \n",
       "\n",
       "                       text  \n",
       "count                681284  \n",
       "unique               611652  \n",
       "top              urlLink     \n",
       "freq                    445  \n",
       "mean                    NaN  \n",
       "std                     NaN  \n",
       "min                     NaN  \n",
       "25%                     NaN  \n",
       "50%                     NaN  \n",
       "75%                     NaN  \n",
       "max                     NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d5394b-7494-4ad4-8f5d-1fc28d19b37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790123\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "maximum = 0 \n",
    "minimum = 100000\n",
    "for text in dataset[\"text\"]:\n",
    "    curr = len(text)\n",
    "    #print(curr)\n",
    "    if curr > maximum:\n",
    "        maximum = curr \n",
    "    if curr < minimum:\n",
    "        minimum = curr\n",
    "    \n",
    "print(maximum)\n",
    "print(minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720a33b-78c3-4058-90e4-bc6d35cfad0d",
   "metadata": {},
   "source": [
    "# Trimming the dataset for quick feedback : remove this code please!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa269ad9-4ada-4088-9903-c49497d9bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[ : 2000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b5f20c-6aae-4f39-8938-d0ccaa5ce5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x2160 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(5, 1, figsize = (20, 30))\n",
    "for i, col in enumerate([\"gender\", \"age\", \"topic\", \"sign\", \"date\"]):\n",
    "    sns.countplot(data = dataset, x = col, ax = axes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e6e7702-a355-4ad1-9bca-4716192e03ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"blogtext.csv\")\n",
    "lens = dataset[\"text\"].apply(lambda x : len(x))\n",
    "sns.histplot(sorted(lens)[ : -1000], bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81a05587-1999-447f-8238-003c6c3aab2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accounting</th>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>605</td>\n",
       "      <td>3693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Advertising</th>\n",
       "      <td>145</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>643</td>\n",
       "      <td>4653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agriculture</th>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>423</td>\n",
       "      <td>1231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Architecture</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>301</td>\n",
       "      <td>1604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arts</th>\n",
       "      <td>721</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>1198</td>\n",
       "      <td>30918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Automotive</th>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>240</td>\n",
       "      <td>1242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Banking</th>\n",
       "      <td>112</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>530</td>\n",
       "      <td>3999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Biotech</th>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>313</td>\n",
       "      <td>2224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BusinessServices</th>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>434</td>\n",
       "      <td>4457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chemicals</th>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>370</td>\n",
       "      <td>3899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Communications-Media</th>\n",
       "      <td>479</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>999</td>\n",
       "      <td>19743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Construction</th>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>180</td>\n",
       "      <td>1064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consulting</th>\n",
       "      <td>191</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>830</td>\n",
       "      <td>5801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>980</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>922</td>\n",
       "      <td>29321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engineering</th>\n",
       "      <td>312</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>913</td>\n",
       "      <td>11340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Environment</th>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>151</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fashion</th>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>620</td>\n",
       "      <td>4804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Government</th>\n",
       "      <td>236</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>525</td>\n",
       "      <td>6792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HumanResources</th>\n",
       "      <td>94</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>367</td>\n",
       "      <td>2999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Internet</th>\n",
       "      <td>397</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>1079</td>\n",
       "      <td>15774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InvestmentBanking</th>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>156</td>\n",
       "      <td>1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Law</th>\n",
       "      <td>197</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>862</td>\n",
       "      <td>8876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LawEnforcement-Security</th>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>603</td>\n",
       "      <td>1858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manufacturing</th>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>440</td>\n",
       "      <td>2210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maritime</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>96</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marketing</th>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>465</td>\n",
       "      <td>4717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Military</th>\n",
       "      <td>116</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>388</td>\n",
       "      <td>3084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Museums-Libraries</th>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>441</td>\n",
       "      <td>3090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-Profit</th>\n",
       "      <td>372</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>798</td>\n",
       "      <td>14580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publishing</th>\n",
       "      <td>150</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>785</td>\n",
       "      <td>7674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RealEstate</th>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>593</td>\n",
       "      <td>2860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Religion</th>\n",
       "      <td>139</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>620</td>\n",
       "      <td>5172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science</th>\n",
       "      <td>184</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>723</td>\n",
       "      <td>7177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports-Recreation</th>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>485</td>\n",
       "      <td>3025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Student</th>\n",
       "      <td>5120</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>1621</td>\n",
       "      <td>144467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Technology</th>\n",
       "      <td>943</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>1264</td>\n",
       "      <td>39011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Telecommunications</th>\n",
       "      <td>119</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>479</td>\n",
       "      <td>3820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tourism</th>\n",
       "      <td>94</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>344</td>\n",
       "      <td>1858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transportation</th>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>255</td>\n",
       "      <td>2168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indUnk</th>\n",
       "      <td>6827</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>1768</td>\n",
       "      <td>240092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  gender  age  sign  date    text\n",
       "topic                                                         \n",
       "Accounting                105       2   20    12   605    3693\n",
       "Advertising               145       2   21    12   643    4653\n",
       "Agriculture                36       2   14    12   423    1231\n",
       "Architecture               69       2   17    12   301    1604\n",
       "Arts                      721       2   26    12  1198   30918\n",
       "Automotive                 54       2   17    12   240    1242\n",
       "Banking                   112       2   17    12   530    3999\n",
       "Biotech                    57       2   16    12   313    2224\n",
       "BusinessServices          163       2   22    12   434    4457\n",
       "Chemicals                  62       2   15    12   370    3899\n",
       "Communications-Media      479       2   26    12   999   19743\n",
       "Construction               55       2   19    12   180    1064\n",
       "Consulting                191       2   25    12   830    5801\n",
       "Education                 980       2   26    12   922   29321\n",
       "Engineering               312       2   23    12   913   11340\n",
       "Environment                28       2   12    10   151     586\n",
       "Fashion                    98       2   19    12   620    4804\n",
       "Government                236       2   25    12   525    6792\n",
       "HumanResources             94       2   19    12   367    2999\n",
       "Internet                  397       2   26    12  1079   15774\n",
       "InvestmentBanking          33       2   13    10   156    1278\n",
       "Law                       197       2   26    12   862    8876\n",
       "LawEnforcement-Security    57       2   14    12   603    1858\n",
       "Manufacturing              87       2   21    12   440    2210\n",
       "Maritime                   17       2   11    10    96     277\n",
       "Marketing                 180       2   21    12   465    4717\n",
       "Military                  116       2   20    12   388    3084\n",
       "Museums-Libraries          55       2   20    12   441    3090\n",
       "Non-Profit                372       2   26    12   798   14580\n",
       "Publishing                150       2   23    12   785    7674\n",
       "RealEstate                 55       2   17    12   593    2860\n",
       "Religion                  139       2   23    12   620    5172\n",
       "Science                   184       2   22    12   723    7177\n",
       "Sports-Recreation          90       2   18    12   485    3025\n",
       "Student                  5120       2   23    12  1621  144467\n",
       "Technology                943       2   26    12  1264   39011\n",
       "Telecommunications        119       2   21    12   479    3820\n",
       "Tourism                    94       2   21    12   344    1858\n",
       "Transportation             91       2   23    12   255    2168\n",
       "indUnk                   6827       2   26    12  1768  240092"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby(\"topic\").nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fee3a2-db3d-425d-8894-4b9c680c8b9b",
   "metadata": {},
   "source": [
    "_Outcome of data analysis_:\n",
    "- Most articles concentrated on few topics\n",
    "- A lot of the blogs are from just a few dates\n",
    "- The maximum length of the articles is more than __700,000__! Need to trim the length\n",
    "- The minimum length of the articles is __4__, need to pad as well\n",
    "- The length of all the articles is an exponential distribution..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ef126-4ff4-4f59-9594-24f603e39f6b",
   "metadata": {},
   "source": [
    "__B. Clean the Structured Data [3 Marks]__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b382065-00a3-43b8-9c03-ced9ef2aa96a",
   "metadata": {},
   "source": [
    "__i. Missing value analysis and imputation. [1 Marks]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a9958ba-9db7-4836-bb3b-da64ed712fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 676598 entries, 0 to 681283\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   id      676598 non-null  int64 \n",
      " 1   gender  676598 non-null  object\n",
      " 2   age     676598 non-null  int64 \n",
      " 3   topic   676598 non-null  object\n",
      " 4   sign    676598 non-null  object\n",
      " 5   date    676598 non-null  object\n",
      " 6   text    676598 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 41.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.drop_duplicates()\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26944b-06a5-41c5-910f-1f3a79c139b6",
   "metadata": {},
   "source": [
    "__ii. Eliminate Non-English textual data. [2 Marks]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d553c261-ea36-474e-b622-66f43ee43cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_16080\\84117258.py:1: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  dataset = dataset.drop(dataset[\"text\"].index[detect(str(dataset[\"text\"])) != \"en\"])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = dataset.drop(dataset[\"text\"].index[detect(str(dataset[\"text\"])) != \"en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f0efa8-5b31-4466-83c9-45c16e71e4fe",
   "metadata": {},
   "source": [
    "#### 2. Preprocess unstructured data to make it consumable for model training. [5 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c4388-fd1e-4223-a1c1-4d1a3bc7e28b",
   "metadata": {},
   "source": [
    "__A. Eliminate All special Characters and Numbers [2 Marks]__\n",
    "\n",
    "__B. Lowercase all textual data [1 Marks]__\n",
    "\n",
    "__C. Remove all Stopwords [1 Marks]__\n",
    "\n",
    "__D. Remove all extra white spaces [1 Marks]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "107a2556-08d0-428f-8d0b-c331ee2216bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_input(text, trim_text = False, length = 500):\n",
    "    # Select only alphabets\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Strip unwanted spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    # Remove stopwords\n",
    "    Stopwords = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in Stopwords])\n",
    "    if trim_text:\n",
    "        text = text[ : length]\n",
    "    if len(text) < length:\n",
    "        text = text + (\"!\" * (length - len(text)))\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32eb1f80-cfaf-4b92-aeb8-ea5e6513c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accf1626-46e6-40e0-ae80-a7953177a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"text\"] = dataset[\"text\"].apply(lambda text : preprocess_input(text, trim_text = True, length = length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d963cd-5ad6-4acc-9c39-451ae0ed3526",
   "metadata": {},
   "source": [
    "#### 3. Build a base Classification model [8 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0fe7ac-a3c9-404b-8e5f-2b238761bc33",
   "metadata": {},
   "source": [
    "__A. Create dependent and independent variables [2 Marks]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d0f0285-5de5-4824-9b57-d21613b046ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop([\"gender\", \"age\", \"sign\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93dafe3e-864d-4096-bf2f-3679292838bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop([\"date\", \"id\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fa4e4c2-2f46-4f96-87c2-735951dccdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student</td>\n",
       "      <td>info found pages mb pdf files wait untill team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student</td>\n",
       "      <td>team members drewes van der laag urllink mail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student</td>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student</td>\n",
       "      <td>testing testing!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>thanks yahoo toolbar capture urls popups means...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>interesting conversation dad morning talking k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>somehow coca cola way summing things well earl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>anything korea country extremes everything see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>take read news article urllink joongang ilbo n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>surf english news sites lot looking tidbits ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>ah korean language looks difficult first figur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>click profile make startling discovery born ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>last night pretty fun mostly company kept rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>much different anything ever seen well travell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>urllink superfantastic phonebox today great da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>one thing love seoul mean korea general happen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>urllink wonderful oh gyup sal favorite pork re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>latest korean rumor mill made way coquitlam wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>well stand corrected yesterday blogged coquitl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>vancouver days coquitlam actually really inter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               labels                                               text\n",
       "0             Student  info found pages mb pdf files wait untill team...\n",
       "1             Student  team members drewes van der laag urllink mail ...\n",
       "2             Student  het kader van kernfusie op aarde maak je eigen...\n",
       "3             Student  testing testing!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...\n",
       "4   InvestmentBanking  thanks yahoo toolbar capture urls popups means...\n",
       "5   InvestmentBanking  interesting conversation dad morning talking k...\n",
       "6   InvestmentBanking  somehow coca cola way summing things well earl...\n",
       "7   InvestmentBanking  anything korea country extremes everything see...\n",
       "8   InvestmentBanking  take read news article urllink joongang ilbo n...\n",
       "9   InvestmentBanking  surf english news sites lot looking tidbits ko...\n",
       "10  InvestmentBanking  ah korean language looks difficult first figur...\n",
       "11  InvestmentBanking  click profile make startling discovery born ye...\n",
       "12  InvestmentBanking  last night pretty fun mostly company kept rece...\n",
       "13  InvestmentBanking  much different anything ever seen well travell...\n",
       "14  InvestmentBanking  urllink superfantastic phonebox today great da...\n",
       "15  InvestmentBanking  one thing love seoul mean korea general happen...\n",
       "16  InvestmentBanking  urllink wonderful oh gyup sal favorite pork re...\n",
       "17  InvestmentBanking  latest korean rumor mill made way coquitlam wi...\n",
       "18  InvestmentBanking  well stand corrected yesterday blogged coquitl...\n",
       "19  InvestmentBanking  vancouver days coquitlam actually really inter..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename(columns = {\"topic\" : \"labels\"})\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf5fb630-d1d8-44ec-8658-4c0ae2b0fea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(676598, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e13ab0-a407-495c-ba26-b17119ec49f4",
   "metadata": {},
   "source": [
    "#### B. Split data into train and test. [1 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7f1d9b0-0838-4cac-a348-8eaf5c717131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.to_hdf(\"./preprocessed_data.hdf5\", \"preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c72e62a3-0dd8-4d54-a3ec-52fb4765c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_hdf(\"./preprocessed_data.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2a714cd-2192-4ef3-8a66-148a660350fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student</td>\n",
       "      <td>info found pages mb pdf files wait untill team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student</td>\n",
       "      <td>team members drewes van der laag urllink mail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student</td>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student</td>\n",
       "      <td>testing testing!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>thanks yahoo toolbar capture urls popups means...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681279</th>\n",
       "      <td>Student</td>\n",
       "      <td>dear susan could write really bitter diatribe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681280</th>\n",
       "      <td>Student</td>\n",
       "      <td>dear susan second yeast infection past two mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681281</th>\n",
       "      <td>Student</td>\n",
       "      <td>dear susan boyfriend fucking bald good luck!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681282</th>\n",
       "      <td>Student</td>\n",
       "      <td>dear susan clarify asking leave house shit pis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681283</th>\n",
       "      <td>Student</td>\n",
       "      <td>hey everybody susan might already know name we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>676598 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   labels                                               text\n",
       "0                 Student  info found pages mb pdf files wait untill team...\n",
       "1                 Student  team members drewes van der laag urllink mail ...\n",
       "2                 Student  het kader van kernfusie op aarde maak je eigen...\n",
       "3                 Student  testing testing!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...\n",
       "4       InvestmentBanking  thanks yahoo toolbar capture urls popups means...\n",
       "...                   ...                                                ...\n",
       "681279            Student  dear susan could write really bitter diatribe ...\n",
       "681280            Student  dear susan second yeast infection past two mon...\n",
       "681281            Student  dear susan boyfriend fucking bald good luck!!!...\n",
       "681282            Student  dear susan clarify asking leave house shit pis...\n",
       "681283            Student  hey everybody susan might already know name we...\n",
       "\n",
       "[676598 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98c8d519-4766-4ec0-87d0-2a0d2db74bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset.text.values, \n",
    "                                                    dataset.labels.values, \n",
    "                                                    test_size = 0.20, \n",
    "                                                    #stratify = True,\n",
    "                                                    random_state = 42\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41de87fe-53e5-4397-b606-cc81eafd54b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Student': 153080,\n",
       " 'InvestmentBanking': 1286,\n",
       " 'indUnk': 248738,\n",
       " 'Non-Profit': 14618,\n",
       " 'Banking': 4033,\n",
       " 'Education': 29572,\n",
       " 'Engineering': 11561,\n",
       " 'Science': 7207,\n",
       " 'Communications-Media': 20038,\n",
       " 'BusinessServices': 4476,\n",
       " 'Sports-Recreation': 3029,\n",
       " 'Arts': 32305,\n",
       " 'Internet': 15884,\n",
       " 'Museums-Libraries': 3095,\n",
       " 'Accounting': 3703,\n",
       " 'Technology': 41913,\n",
       " 'Law': 9022,\n",
       " 'Consulting': 5823,\n",
       " 'Automotive': 1242,\n",
       " 'Religion': 5188,\n",
       " 'Fashion': 4824,\n",
       " 'Publishing': 7710,\n",
       " 'Marketing': 4757,\n",
       " 'LawEnforcement-Security': 1875,\n",
       " 'HumanResources': 3003,\n",
       " 'Telecommunications': 3842,\n",
       " 'Military': 3093,\n",
       " 'Government': 6866,\n",
       " 'Transportation': 2185,\n",
       " 'Architecture': 1611,\n",
       " 'Advertising': 4663,\n",
       " 'Agriculture': 1234,\n",
       " 'Biotech': 2229,\n",
       " 'RealEstate': 2867,\n",
       " 'Manufacturing': 2231,\n",
       " 'Construction': 1075,\n",
       " 'Chemicals': 3919,\n",
       " 'Maritime': 277,\n",
       " 'Tourism': 1936,\n",
       " 'Environment': 588}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = dict()\n",
    "\n",
    "for label in dataset[\"labels\"]:\n",
    "    if label in label_counts:\n",
    "        label_counts[label] += 1\n",
    "    else:\n",
    "        label_counts[label] = 1\n",
    "        \n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7972101e-8c78-4d52-970c-43ec97e259ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((541278, 40), (135320, 40))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse = False)\n",
    "\n",
    "\n",
    "train_labels = ohe.fit_transform(y_train.reshape(-1,1))\n",
    "test_labels = ohe.transform(y_test.reshape(-1,1))\n",
    "\n",
    "\n",
    "train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "933b9815-7a34-433f-82ae-f64cf98f5821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((541278,), (135320,), (541278,), (135320,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0fc6e-4d15-47a9-954a-08f3aad1740d",
   "metadata": {},
   "source": [
    "with h5py.File(\"./train_data.hdf5\", \"w\") as f:\n",
    "    train_data = f.create_dataset(\"train_data\", (X_train.shape[0], 500, 25), \"float16\")\n",
    "\n",
    "with h5py.File(\"./test_data.hdf5\", \"w\") as f:\n",
    "    test_data = f.create_dataset(\"test_data\", (X_test.shape[0], 500, 25), \"float16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cad5193-1da4-452f-a1e0-73222ff402f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e92733-948b-471e-8c00-d80145bb9c3e",
   "metadata": {},
   "source": [
    "#### C. Vectorize data using any one vectorizer. [2 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66a2c89a-6d90-43b9-b4c7-1b58da7b078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings_dict = {}\n",
    "with open(\"glove.twitter.27B.25d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float16\")\n",
    "        embeddings_dict[word] = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cbf3028-d57e-4f8f-b7b7-8049cdf6c13f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached at record 0\n",
      "reached at record 1000\n",
      "reached at record 2000\n",
      "reached at record 3000\n",
      "reached at record 4000\n",
      "reached at record 5000\n",
      "reached at record 6000\n",
      "reached at record 7000\n",
      "reached at record 8000\n",
      "reached at record 9000\n",
      "reached at record 10000\n",
      "reached at record 11000\n",
      "reached at record 12000\n",
      "reached at record 13000\n",
      "reached at record 14000\n",
      "reached at record 15000\n",
      "reached at record 16000\n",
      "reached at record 17000\n",
      "reached at record 18000\n",
      "reached at record 19000\n",
      "reached at record 20000\n",
      "reached at record 21000\n",
      "reached at record 22000\n",
      "reached at record 23000\n",
      "reached at record 24000\n",
      "reached at record 25000\n",
      "reached at record 26000\n",
      "reached at record 27000\n",
      "reached at record 28000\n",
      "reached at record 29000\n",
      "reached at record 30000\n",
      "reached at record 31000\n",
      "reached at record 32000\n",
      "reached at record 33000\n",
      "reached at record 34000\n",
      "reached at record 35000\n",
      "reached at record 36000\n",
      "reached at record 37000\n",
      "reached at record 38000\n",
      "reached at record 39000\n",
      "reached at record 40000\n",
      "reached at record 41000\n",
      "reached at record 42000\n",
      "reached at record 43000\n",
      "reached at record 44000\n",
      "reached at record 45000\n",
      "reached at record 46000\n",
      "reached at record 47000\n",
      "reached at record 48000\n",
      "reached at record 49000\n",
      "reached at record 50000\n",
      "reached at record 51000\n",
      "reached at record 52000\n",
      "reached at record 53000\n",
      "reached at record 54000\n",
      "reached at record 55000\n",
      "reached at record 56000\n",
      "reached at record 57000\n",
      "reached at record 58000\n",
      "reached at record 59000\n",
      "reached at record 60000\n",
      "reached at record 61000\n",
      "reached at record 62000\n",
      "reached at record 63000\n",
      "reached at record 64000\n",
      "reached at record 65000\n",
      "reached at record 66000\n",
      "reached at record 67000\n",
      "reached at record 68000\n",
      "reached at record 69000\n",
      "reached at record 70000\n",
      "reached at record 71000\n",
      "reached at record 72000\n",
      "reached at record 73000\n",
      "reached at record 74000\n",
      "reached at record 75000\n",
      "reached at record 76000\n",
      "reached at record 77000\n",
      "reached at record 78000\n",
      "reached at record 79000\n",
      "reached at record 80000\n",
      "reached at record 81000\n",
      "reached at record 82000\n",
      "reached at record 83000\n",
      "reached at record 84000\n",
      "reached at record 85000\n",
      "reached at record 86000\n",
      "reached at record 87000\n",
      "reached at record 88000\n",
      "reached at record 89000\n",
      "reached at record 90000\n",
      "reached at record 91000\n",
      "reached at record 92000\n",
      "reached at record 93000\n",
      "reached at record 94000\n",
      "reached at record 95000\n",
      "reached at record 96000\n",
      "reached at record 97000\n",
      "reached at record 98000\n",
      "reached at record 99000\n",
      "reached at record 100000\n",
      "reached at record 101000\n",
      "reached at record 102000\n",
      "reached at record 103000\n",
      "reached at record 104000\n",
      "reached at record 105000\n",
      "reached at record 106000\n",
      "reached at record 107000\n",
      "reached at record 108000\n",
      "reached at record 109000\n",
      "reached at record 110000\n",
      "reached at record 111000\n",
      "reached at record 112000\n",
      "reached at record 113000\n",
      "reached at record 114000\n",
      "reached at record 115000\n",
      "reached at record 116000\n",
      "reached at record 117000\n",
      "reached at record 118000\n",
      "reached at record 119000\n",
      "reached at record 120000\n",
      "reached at record 121000\n",
      "reached at record 122000\n",
      "reached at record 123000\n",
      "reached at record 124000\n",
      "reached at record 125000\n",
      "reached at record 126000\n",
      "reached at record 127000\n",
      "reached at record 128000\n",
      "reached at record 129000\n",
      "reached at record 130000\n",
      "reached at record 131000\n",
      "reached at record 132000\n",
      "reached at record 133000\n",
      "reached at record 134000\n",
      "reached at record 135000\n",
      "reached at record 136000\n",
      "reached at record 137000\n",
      "reached at record 138000\n",
      "reached at record 139000\n",
      "reached at record 140000\n",
      "reached at record 141000\n",
      "reached at record 142000\n",
      "reached at record 143000\n",
      "reached at record 144000\n",
      "reached at record 145000\n",
      "reached at record 146000\n",
      "reached at record 147000\n",
      "reached at record 148000\n",
      "reached at record 149000\n",
      "reached at record 150000\n",
      "reached at record 151000\n",
      "reached at record 152000\n",
      "reached at record 153000\n",
      "reached at record 154000\n",
      "reached at record 155000\n",
      "reached at record 156000\n",
      "reached at record 157000\n",
      "reached at record 158000\n",
      "reached at record 159000\n",
      "reached at record 160000\n",
      "reached at record 161000\n",
      "reached at record 162000\n",
      "reached at record 163000\n",
      "reached at record 164000\n",
      "reached at record 165000\n",
      "reached at record 166000\n",
      "reached at record 167000\n",
      "reached at record 168000\n",
      "reached at record 169000\n",
      "reached at record 170000\n",
      "reached at record 171000\n",
      "reached at record 172000\n",
      "reached at record 173000\n",
      "reached at record 174000\n",
      "reached at record 175000\n",
      "reached at record 176000\n",
      "reached at record 177000\n",
      "reached at record 178000\n",
      "reached at record 179000\n",
      "reached at record 180000\n",
      "reached at record 181000\n",
      "reached at record 182000\n",
      "reached at record 183000\n",
      "reached at record 184000\n",
      "reached at record 185000\n",
      "reached at record 186000\n",
      "reached at record 187000\n",
      "reached at record 188000\n",
      "reached at record 189000\n",
      "reached at record 190000\n",
      "reached at record 191000\n",
      "reached at record 192000\n",
      "reached at record 193000\n",
      "reached at record 194000\n",
      "reached at record 195000\n",
      "reached at record 196000\n",
      "reached at record 197000\n",
      "reached at record 198000\n",
      "reached at record 199000\n",
      "reached at record 200000\n",
      "reached at record 201000\n",
      "reached at record 202000\n",
      "reached at record 203000\n",
      "reached at record 204000\n",
      "reached at record 205000\n",
      "reached at record 206000\n",
      "reached at record 207000\n",
      "reached at record 208000\n",
      "reached at record 209000\n",
      "reached at record 210000\n",
      "reached at record 211000\n",
      "reached at record 212000\n",
      "reached at record 213000\n",
      "reached at record 214000\n",
      "reached at record 215000\n",
      "reached at record 216000\n",
      "reached at record 217000\n",
      "reached at record 218000\n",
      "reached at record 219000\n",
      "reached at record 220000\n",
      "reached at record 221000\n",
      "reached at record 222000\n",
      "reached at record 223000\n",
      "reached at record 224000\n",
      "reached at record 225000\n",
      "reached at record 226000\n",
      "reached at record 227000\n",
      "reached at record 228000\n",
      "reached at record 229000\n",
      "reached at record 230000\n",
      "reached at record 231000\n",
      "reached at record 232000\n",
      "reached at record 233000\n",
      "reached at record 234000\n",
      "reached at record 235000\n",
      "reached at record 236000\n",
      "reached at record 237000\n",
      "reached at record 238000\n",
      "reached at record 239000\n",
      "reached at record 240000\n",
      "reached at record 241000\n",
      "reached at record 242000\n",
      "reached at record 243000\n",
      "reached at record 244000\n",
      "reached at record 245000\n",
      "reached at record 246000\n",
      "reached at record 247000\n",
      "reached at record 248000\n",
      "reached at record 249000\n",
      "reached at record 250000\n",
      "reached at record 251000\n",
      "reached at record 252000\n",
      "reached at record 253000\n",
      "reached at record 254000\n",
      "reached at record 255000\n",
      "reached at record 256000\n",
      "reached at record 257000\n",
      "reached at record 258000\n",
      "reached at record 259000\n",
      "reached at record 260000\n",
      "reached at record 261000\n",
      "reached at record 262000\n",
      "reached at record 263000\n",
      "reached at record 264000\n",
      "reached at record 265000\n",
      "reached at record 266000\n",
      "reached at record 267000\n",
      "reached at record 268000\n",
      "reached at record 269000\n",
      "reached at record 270000\n",
      "reached at record 271000\n",
      "reached at record 272000\n",
      "reached at record 273000\n",
      "reached at record 274000\n",
      "reached at record 275000\n",
      "reached at record 276000\n",
      "reached at record 277000\n",
      "reached at record 278000\n",
      "reached at record 279000\n",
      "reached at record 280000\n",
      "reached at record 281000\n",
      "reached at record 282000\n",
      "reached at record 283000\n",
      "reached at record 284000\n",
      "reached at record 285000\n",
      "reached at record 286000\n",
      "reached at record 287000\n",
      "reached at record 288000\n",
      "reached at record 289000\n",
      "reached at record 290000\n",
      "reached at record 291000\n",
      "reached at record 292000\n",
      "reached at record 293000\n",
      "reached at record 294000\n",
      "reached at record 295000\n",
      "reached at record 296000\n",
      "reached at record 297000\n",
      "reached at record 298000\n",
      "reached at record 299000\n",
      "reached at record 300000\n",
      "reached at record 301000\n",
      "reached at record 302000\n",
      "reached at record 303000\n",
      "reached at record 304000\n",
      "reached at record 305000\n",
      "reached at record 306000\n",
      "reached at record 307000\n",
      "reached at record 308000\n",
      "reached at record 309000\n",
      "reached at record 310000\n",
      "reached at record 311000\n",
      "reached at record 312000\n",
      "reached at record 313000\n",
      "reached at record 314000\n",
      "reached at record 315000\n",
      "reached at record 316000\n",
      "reached at record 317000\n",
      "reached at record 318000\n",
      "reached at record 319000\n",
      "reached at record 320000\n",
      "reached at record 321000\n",
      "reached at record 322000\n",
      "reached at record 323000\n",
      "reached at record 324000\n",
      "reached at record 325000\n",
      "reached at record 326000\n",
      "reached at record 327000\n",
      "reached at record 328000\n",
      "reached at record 329000\n",
      "reached at record 330000\n",
      "reached at record 331000\n",
      "reached at record 332000\n",
      "reached at record 333000\n",
      "reached at record 334000\n",
      "reached at record 335000\n",
      "reached at record 336000\n",
      "reached at record 337000\n",
      "reached at record 338000\n",
      "reached at record 339000\n",
      "reached at record 340000\n",
      "reached at record 341000\n",
      "reached at record 342000\n",
      "reached at record 343000\n",
      "reached at record 344000\n",
      "reached at record 345000\n",
      "reached at record 346000\n",
      "reached at record 347000\n",
      "reached at record 348000\n",
      "reached at record 349000\n",
      "reached at record 350000\n",
      "reached at record 351000\n",
      "reached at record 352000\n",
      "reached at record 353000\n",
      "reached at record 354000\n",
      "reached at record 355000\n",
      "reached at record 356000\n",
      "reached at record 357000\n",
      "reached at record 358000\n",
      "reached at record 359000\n",
      "reached at record 360000\n",
      "reached at record 361000\n",
      "reached at record 362000\n",
      "reached at record 363000\n",
      "reached at record 364000\n",
      "reached at record 365000\n",
      "reached at record 366000\n",
      "reached at record 367000\n",
      "reached at record 368000\n",
      "reached at record 369000\n",
      "reached at record 370000\n",
      "reached at record 371000\n",
      "reached at record 372000\n",
      "reached at record 373000\n",
      "reached at record 374000\n",
      "reached at record 375000\n",
      "reached at record 376000\n",
      "reached at record 377000\n",
      "reached at record 378000\n",
      "reached at record 379000\n",
      "reached at record 380000\n",
      "reached at record 381000\n",
      "reached at record 382000\n",
      "reached at record 383000\n",
      "reached at record 384000\n",
      "reached at record 385000\n",
      "reached at record 386000\n",
      "reached at record 387000\n",
      "reached at record 388000\n",
      "reached at record 389000\n",
      "reached at record 390000\n",
      "reached at record 391000\n",
      "reached at record 392000\n",
      "reached at record 393000\n",
      "reached at record 394000\n",
      "reached at record 395000\n",
      "reached at record 396000\n",
      "reached at record 397000\n",
      "reached at record 398000\n",
      "reached at record 399000\n",
      "reached at record 400000\n",
      "reached at record 401000\n",
      "reached at record 402000\n",
      "reached at record 403000\n",
      "reached at record 404000\n",
      "reached at record 405000\n",
      "reached at record 406000\n",
      "reached at record 407000\n",
      "reached at record 408000\n",
      "reached at record 409000\n",
      "reached at record 410000\n",
      "reached at record 411000\n",
      "reached at record 412000\n",
      "reached at record 413000\n",
      "reached at record 414000\n",
      "reached at record 415000\n",
      "reached at record 416000\n",
      "reached at record 417000\n",
      "reached at record 418000\n",
      "reached at record 419000\n",
      "reached at record 420000\n",
      "reached at record 421000\n",
      "reached at record 422000\n",
      "reached at record 423000\n",
      "reached at record 424000\n",
      "reached at record 425000\n",
      "reached at record 426000\n",
      "reached at record 427000\n",
      "reached at record 428000\n",
      "reached at record 429000\n",
      "reached at record 430000\n",
      "reached at record 431000\n",
      "reached at record 432000\n",
      "reached at record 433000\n",
      "reached at record 434000\n",
      "reached at record 435000\n",
      "reached at record 436000\n",
      "reached at record 437000\n",
      "reached at record 438000\n",
      "reached at record 439000\n",
      "reached at record 440000\n",
      "reached at record 441000\n",
      "reached at record 442000\n",
      "reached at record 443000\n",
      "reached at record 444000\n",
      "reached at record 445000\n",
      "reached at record 446000\n",
      "reached at record 447000\n",
      "reached at record 448000\n",
      "reached at record 449000\n",
      "reached at record 450000\n",
      "reached at record 451000\n",
      "reached at record 452000\n",
      "reached at record 453000\n",
      "reached at record 454000\n",
      "reached at record 455000\n",
      "reached at record 456000\n",
      "reached at record 457000\n",
      "reached at record 458000\n",
      "reached at record 459000\n",
      "reached at record 460000\n",
      "reached at record 461000\n",
      "reached at record 462000\n",
      "reached at record 463000\n",
      "reached at record 464000\n",
      "reached at record 465000\n",
      "reached at record 466000\n",
      "reached at record 467000\n",
      "reached at record 468000\n",
      "reached at record 469000\n",
      "reached at record 470000\n",
      "reached at record 471000\n",
      "reached at record 472000\n",
      "reached at record 473000\n",
      "reached at record 474000\n",
      "reached at record 475000\n",
      "reached at record 476000\n",
      "reached at record 477000\n",
      "reached at record 478000\n",
      "reached at record 479000\n",
      "reached at record 480000\n",
      "reached at record 481000\n",
      "reached at record 482000\n",
      "reached at record 483000\n",
      "reached at record 484000\n",
      "reached at record 485000\n",
      "reached at record 486000\n",
      "reached at record 487000\n",
      "reached at record 488000\n",
      "reached at record 489000\n",
      "reached at record 490000\n",
      "reached at record 491000\n",
      "reached at record 492000\n",
      "reached at record 493000\n",
      "reached at record 494000\n",
      "reached at record 495000\n",
      "reached at record 496000\n",
      "reached at record 497000\n",
      "reached at record 498000\n",
      "reached at record 499000\n",
      "reached at record 500000\n",
      "reached at record 501000\n",
      "reached at record 502000\n",
      "reached at record 503000\n",
      "reached at record 504000\n",
      "reached at record 505000\n",
      "reached at record 506000\n",
      "reached at record 507000\n",
      "reached at record 508000\n",
      "reached at record 509000\n",
      "reached at record 510000\n",
      "reached at record 511000\n",
      "reached at record 512000\n",
      "reached at record 513000\n",
      "reached at record 514000\n",
      "reached at record 515000\n",
      "reached at record 516000\n",
      "reached at record 517000\n",
      "reached at record 518000\n",
      "reached at record 519000\n",
      "reached at record 520000\n",
      "reached at record 521000\n",
      "reached at record 522000\n",
      "reached at record 523000\n",
      "reached at record 524000\n",
      "reached at record 525000\n",
      "reached at record 526000\n",
      "reached at record 527000\n",
      "reached at record 528000\n",
      "reached at record 529000\n",
      "reached at record 530000\n",
      "reached at record 531000\n",
      "reached at record 532000\n",
      "reached at record 533000\n",
      "reached at record 534000\n",
      "reached at record 535000\n",
      "reached at record 536000\n",
      "reached at record 537000\n",
      "reached at record 538000\n",
      "reached at record 539000\n",
      "reached at record 540000\n",
      "reached at record 541000\n",
      "reached at record 0\n",
      "reached at record 1000\n",
      "reached at record 2000\n",
      "reached at record 3000\n",
      "reached at record 4000\n",
      "reached at record 5000\n",
      "reached at record 6000\n",
      "reached at record 7000\n",
      "reached at record 8000\n",
      "reached at record 9000\n",
      "reached at record 10000\n",
      "reached at record 11000\n",
      "reached at record 12000\n",
      "reached at record 13000\n",
      "reached at record 14000\n",
      "reached at record 15000\n",
      "reached at record 16000\n",
      "reached at record 17000\n",
      "reached at record 18000\n",
      "reached at record 19000\n",
      "reached at record 20000\n",
      "reached at record 21000\n",
      "reached at record 22000\n",
      "reached at record 23000\n",
      "reached at record 24000\n",
      "reached at record 25000\n",
      "reached at record 26000\n",
      "reached at record 27000\n",
      "reached at record 28000\n",
      "reached at record 29000\n",
      "reached at record 30000\n",
      "reached at record 31000\n",
      "reached at record 32000\n",
      "reached at record 33000\n",
      "reached at record 34000\n",
      "reached at record 35000\n",
      "reached at record 36000\n",
      "reached at record 37000\n",
      "reached at record 38000\n",
      "reached at record 39000\n",
      "reached at record 40000\n",
      "reached at record 41000\n",
      "reached at record 42000\n",
      "reached at record 43000\n",
      "reached at record 44000\n",
      "reached at record 45000\n",
      "reached at record 46000\n",
      "reached at record 47000\n",
      "reached at record 48000\n",
      "reached at record 49000\n",
      "reached at record 50000\n",
      "reached at record 51000\n",
      "reached at record 52000\n",
      "reached at record 53000\n",
      "reached at record 54000\n",
      "reached at record 55000\n",
      "reached at record 56000\n",
      "reached at record 57000\n",
      "reached at record 58000\n",
      "reached at record 59000\n",
      "reached at record 60000\n",
      "reached at record 61000\n",
      "reached at record 62000\n",
      "reached at record 63000\n",
      "reached at record 64000\n",
      "reached at record 65000\n",
      "reached at record 66000\n",
      "reached at record 67000\n",
      "reached at record 68000\n",
      "reached at record 69000\n",
      "reached at record 70000\n",
      "reached at record 71000\n",
      "reached at record 72000\n",
      "reached at record 73000\n",
      "reached at record 74000\n",
      "reached at record 75000\n",
      "reached at record 76000\n",
      "reached at record 77000\n",
      "reached at record 78000\n",
      "reached at record 79000\n",
      "reached at record 80000\n",
      "reached at record 81000\n",
      "reached at record 82000\n",
      "reached at record 83000\n",
      "reached at record 84000\n",
      "reached at record 85000\n",
      "reached at record 86000\n",
      "reached at record 87000\n",
      "reached at record 88000\n",
      "reached at record 89000\n",
      "reached at record 90000\n",
      "reached at record 91000\n",
      "reached at record 92000\n",
      "reached at record 93000\n",
      "reached at record 94000\n",
      "reached at record 95000\n",
      "reached at record 96000\n",
      "reached at record 97000\n",
      "reached at record 98000\n",
      "reached at record 99000\n",
      "reached at record 100000\n",
      "reached at record 101000\n",
      "reached at record 102000\n",
      "reached at record 103000\n",
      "reached at record 104000\n",
      "reached at record 105000\n",
      "reached at record 106000\n",
      "reached at record 107000\n",
      "reached at record 108000\n",
      "reached at record 109000\n",
      "reached at record 110000\n",
      "reached at record 111000\n",
      "reached at record 112000\n",
      "reached at record 113000\n",
      "reached at record 114000\n",
      "reached at record 115000\n",
      "reached at record 116000\n",
      "reached at record 117000\n",
      "reached at record 118000\n",
      "reached at record 119000\n",
      "reached at record 120000\n",
      "reached at record 121000\n",
      "reached at record 122000\n",
      "reached at record 123000\n",
      "reached at record 124000\n",
      "reached at record 125000\n",
      "reached at record 126000\n",
      "reached at record 127000\n",
      "reached at record 128000\n",
      "reached at record 129000\n",
      "reached at record 130000\n",
      "reached at record 131000\n",
      "reached at record 132000\n",
      "reached at record 133000\n",
      "reached at record 134000\n",
      "reached at record 135000\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"./train_data.hdf5\", \"w\") as f:\n",
    "    train_data = f.create_dataset(\"train_data\", (X_train.shape[0], 500, 25), \"float16\", **{\"chunks\" : ( 2048, 500, 25)})\n",
    "    for index, data in enumerate(X_train):\n",
    "        for index_2, word in enumerate(data):\n",
    "            if word == \"!\":\n",
    "                vec = np.zeros(shape =(25,))\n",
    "            else :\n",
    "                vec = embeddings_dict.get(word, np.zeros(shape = (25,)))\n",
    "            train_data[index, index_2] = vec\n",
    "        if index % 1000 == 0:\n",
    "            print(f\"reached at record {index}\")\n",
    "        if index % 10_000 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "with h5py.File(\"./test_data.hdf5\", \"w\") as f:\n",
    "    test_data = f.create_dataset(\"test_data\", (X_test.shape[0], 500, 25), \"float16\", **{\"chunks\" : ( 2048, 500, 25)})\n",
    "    for index, data in enumerate(X_test):\n",
    "        for index_2, word in enumerate(data):\n",
    "            if word == \"!\":\n",
    "                vec = np.zeros(shape =(25,))\n",
    "            else :\n",
    "                vec = embeddings_dict.get(word, np.zeros(shape = (25,)))\n",
    "            test_data[index, index_2] = vec\n",
    "        if index % 1000 == 0:\n",
    "            print(f\"reached at record {index}\")\n",
    "        if index % 10_000 == 0:\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e68343be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"./train_labels.hdf5\", \"w\") as f:\n",
    "    train_labels_ = f.require_dataset(\"train_labels\", shape = (train_labels.shape), dtype = \"int\", data = train_labels)\n",
    "\n",
    "with h5py.File(\"./test_labels.hdf5\", \"w\") as f:\n",
    "    test_labels_ = f.require_dataset(\"test_labels\", shape = (test_labels.shape), dtype = \"int\", data = test_labels) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24234021",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached at record 0\n",
      "reached at record 1000\n",
      "reached at record 2000\n",
      "reached at record 3000\n",
      "reached at record 4000\n",
      "reached at record 5000\n",
      "reached at record 6000\n",
      "reached at record 7000\n",
      "reached at record 8000\n",
      "reached at record 9000\n",
      "reached at record 10000\n",
      "reached at record 11000\n",
      "reached at record 12000\n",
      "reached at record 13000\n",
      "reached at record 14000\n",
      "reached at record 15000\n",
      "reached at record 16000\n",
      "reached at record 17000\n",
      "reached at record 18000\n",
      "reached at record 19000\n",
      "reached at record 20000\n",
      "reached at record 21000\n",
      "reached at record 22000\n",
      "reached at record 23000\n",
      "reached at record 24000\n",
      "reached at record 25000\n",
      "reached at record 26000\n",
      "reached at record 27000\n",
      "reached at record 28000\n",
      "reached at record 29000\n",
      "reached at record 30000\n",
      "reached at record 31000\n",
      "reached at record 32000\n",
      "reached at record 33000\n",
      "reached at record 34000\n",
      "reached at record 35000\n",
      "reached at record 36000\n",
      "reached at record 37000\n",
      "reached at record 38000\n",
      "reached at record 39000\n",
      "reached at record 40000\n",
      "reached at record 41000\n",
      "reached at record 42000\n",
      "reached at record 43000\n",
      "reached at record 44000\n",
      "reached at record 45000\n",
      "reached at record 46000\n",
      "reached at record 47000\n",
      "reached at record 48000\n",
      "reached at record 49000\n",
      "reached at record 50000\n",
      "reached at record 51000\n",
      "reached at record 52000\n",
      "reached at record 53000\n",
      "reached at record 54000\n",
      "reached at record 55000\n",
      "reached at record 56000\n",
      "reached at record 57000\n",
      "reached at record 58000\n",
      "reached at record 59000\n",
      "reached at record 60000\n",
      "reached at record 61000\n",
      "reached at record 62000\n",
      "reached at record 63000\n",
      "reached at record 64000\n",
      "reached at record 65000\n",
      "reached at record 66000\n",
      "reached at record 67000\n",
      "reached at record 68000\n",
      "reached at record 69000\n",
      "reached at record 70000\n",
      "reached at record 71000\n",
      "reached at record 72000\n",
      "reached at record 73000\n",
      "reached at record 74000\n",
      "reached at record 75000\n",
      "reached at record 76000\n",
      "reached at record 77000\n",
      "reached at record 78000\n",
      "reached at record 79000\n",
      "reached at record 80000\n",
      "reached at record 81000\n",
      "reached at record 82000\n",
      "reached at record 83000\n",
      "reached at record 84000\n",
      "reached at record 85000\n",
      "reached at record 86000\n",
      "reached at record 87000\n",
      "reached at record 88000\n",
      "reached at record 89000\n",
      "reached at record 90000\n",
      "reached at record 91000\n",
      "reached at record 92000\n",
      "reached at record 93000\n",
      "reached at record 94000\n",
      "reached at record 95000\n",
      "reached at record 96000\n",
      "reached at record 97000\n",
      "reached at record 98000\n",
      "reached at record 99000\n",
      "reached at record 100000\n",
      "reached at record 101000\n",
      "reached at record 102000\n",
      "reached at record 103000\n",
      "reached at record 104000\n",
      "reached at record 105000\n",
      "reached at record 106000\n",
      "reached at record 107000\n",
      "reached at record 108000\n",
      "reached at record 109000\n",
      "reached at record 110000\n",
      "reached at record 111000\n",
      "reached at record 112000\n",
      "reached at record 113000\n",
      "reached at record 114000\n",
      "reached at record 115000\n",
      "reached at record 116000\n",
      "reached at record 117000\n",
      "reached at record 118000\n",
      "reached at record 119000\n",
      "reached at record 120000\n",
      "reached at record 121000\n",
      "reached at record 122000\n",
      "reached at record 123000\n",
      "reached at record 124000\n",
      "reached at record 125000\n",
      "reached at record 126000\n",
      "reached at record 127000\n",
      "reached at record 128000\n",
      "reached at record 129000\n",
      "reached at record 130000\n",
      "reached at record 131000\n",
      "reached at record 132000\n",
      "reached at record 133000\n",
      "reached at record 134000\n",
      "reached at record 135000\n",
      "reached at record 136000\n",
      "reached at record 137000\n",
      "reached at record 138000\n",
      "reached at record 139000\n",
      "reached at record 140000\n",
      "reached at record 141000\n",
      "reached at record 142000\n",
      "reached at record 143000\n",
      "reached at record 144000\n",
      "reached at record 145000\n",
      "reached at record 146000\n",
      "reached at record 147000\n",
      "reached at record 148000\n",
      "reached at record 149000\n",
      "reached at record 150000\n",
      "reached at record 151000\n",
      "reached at record 152000\n",
      "reached at record 153000\n",
      "reached at record 154000\n",
      "reached at record 155000\n",
      "reached at record 156000\n",
      "reached at record 157000\n",
      "reached at record 158000\n",
      "reached at record 159000\n",
      "reached at record 160000\n",
      "reached at record 161000\n",
      "reached at record 162000\n",
      "reached at record 163000\n",
      "reached at record 164000\n",
      "reached at record 165000\n",
      "reached at record 166000\n",
      "reached at record 167000\n",
      "reached at record 168000\n",
      "reached at record 169000\n",
      "reached at record 170000\n",
      "reached at record 171000\n",
      "reached at record 172000\n",
      "reached at record 173000\n",
      "reached at record 174000\n",
      "reached at record 175000\n",
      "reached at record 176000\n",
      "reached at record 177000\n",
      "reached at record 178000\n",
      "reached at record 179000\n",
      "reached at record 180000\n",
      "reached at record 181000\n",
      "reached at record 182000\n",
      "reached at record 183000\n",
      "reached at record 184000\n",
      "reached at record 185000\n",
      "reached at record 186000\n",
      "reached at record 187000\n",
      "reached at record 188000\n",
      "reached at record 189000\n",
      "reached at record 190000\n",
      "reached at record 191000\n",
      "reached at record 192000\n",
      "reached at record 193000\n",
      "reached at record 194000\n",
      "reached at record 195000\n",
      "reached at record 196000\n",
      "reached at record 197000\n",
      "reached at record 198000\n",
      "reached at record 199000\n",
      "reached at record 200000\n",
      "reached at record 201000\n",
      "reached at record 202000\n",
      "reached at record 203000\n",
      "reached at record 204000\n",
      "reached at record 205000\n",
      "reached at record 206000\n",
      "reached at record 207000\n",
      "reached at record 208000\n",
      "reached at record 209000\n",
      "reached at record 210000\n",
      "reached at record 211000\n",
      "reached at record 212000\n",
      "reached at record 213000\n",
      "reached at record 214000\n",
      "reached at record 215000\n",
      "reached at record 216000\n",
      "reached at record 217000\n",
      "reached at record 218000\n",
      "reached at record 219000\n",
      "reached at record 220000\n",
      "reached at record 221000\n",
      "reached at record 222000\n",
      "reached at record 223000\n",
      "reached at record 224000\n",
      "reached at record 225000\n",
      "reached at record 226000\n",
      "reached at record 227000\n",
      "reached at record 228000\n",
      "reached at record 229000\n",
      "reached at record 230000\n",
      "reached at record 231000\n",
      "reached at record 232000\n",
      "reached at record 233000\n",
      "reached at record 234000\n",
      "reached at record 235000\n",
      "reached at record 236000\n",
      "reached at record 237000\n",
      "reached at record 238000\n",
      "reached at record 239000\n",
      "reached at record 240000\n",
      "reached at record 241000\n",
      "reached at record 242000\n",
      "reached at record 243000\n",
      "reached at record 244000\n",
      "reached at record 245000\n",
      "reached at record 246000\n",
      "reached at record 247000\n",
      "reached at record 248000\n",
      "reached at record 249000\n",
      "reached at record 250000\n",
      "reached at record 251000\n",
      "reached at record 252000\n",
      "reached at record 253000\n",
      "reached at record 254000\n",
      "reached at record 255000\n",
      "reached at record 256000\n",
      "reached at record 257000\n",
      "reached at record 258000\n",
      "reached at record 259000\n",
      "reached at record 260000\n",
      "reached at record 261000\n",
      "reached at record 262000\n",
      "reached at record 263000\n",
      "reached at record 264000\n",
      "reached at record 265000\n",
      "reached at record 266000\n",
      "reached at record 267000\n",
      "reached at record 268000\n",
      "reached at record 269000\n",
      "reached at record 270000\n",
      "reached at record 271000\n",
      "reached at record 272000\n",
      "reached at record 273000\n",
      "reached at record 274000\n",
      "reached at record 275000\n",
      "reached at record 276000\n",
      "reached at record 277000\n",
      "reached at record 278000\n",
      "reached at record 279000\n",
      "reached at record 280000\n",
      "reached at record 281000\n",
      "reached at record 282000\n",
      "reached at record 283000\n",
      "reached at record 284000\n",
      "reached at record 285000\n",
      "reached at record 286000\n",
      "reached at record 287000\n",
      "reached at record 288000\n",
      "reached at record 289000\n",
      "reached at record 290000\n",
      "reached at record 291000\n",
      "reached at record 292000\n",
      "reached at record 293000\n",
      "reached at record 294000\n",
      "reached at record 295000\n",
      "reached at record 296000\n",
      "reached at record 297000\n",
      "reached at record 298000\n",
      "reached at record 299000\n",
      "reached at record 300000\n",
      "reached at record 301000\n",
      "reached at record 302000\n",
      "reached at record 303000\n",
      "reached at record 304000\n",
      "reached at record 305000\n",
      "reached at record 306000\n",
      "reached at record 307000\n",
      "reached at record 308000\n",
      "reached at record 309000\n",
      "reached at record 310000\n",
      "reached at record 311000\n",
      "reached at record 312000\n",
      "reached at record 313000\n",
      "reached at record 314000\n",
      "reached at record 315000\n",
      "reached at record 316000\n",
      "reached at record 317000\n",
      "reached at record 318000\n",
      "reached at record 319000\n",
      "reached at record 320000\n",
      "reached at record 321000\n",
      "reached at record 322000\n",
      "reached at record 323000\n",
      "reached at record 324000\n",
      "reached at record 325000\n",
      "reached at record 326000\n",
      "reached at record 327000\n",
      "reached at record 328000\n",
      "reached at record 329000\n",
      "reached at record 330000\n",
      "reached at record 331000\n",
      "reached at record 332000\n",
      "reached at record 333000\n",
      "reached at record 334000\n",
      "reached at record 335000\n",
      "reached at record 336000\n",
      "reached at record 337000\n",
      "reached at record 338000\n",
      "reached at record 339000\n",
      "reached at record 340000\n",
      "reached at record 341000\n",
      "reached at record 342000\n",
      "reached at record 343000\n",
      "reached at record 344000\n",
      "reached at record 345000\n",
      "reached at record 346000\n",
      "reached at record 347000\n",
      "reached at record 348000\n",
      "reached at record 349000\n",
      "reached at record 350000\n",
      "reached at record 351000\n",
      "reached at record 352000\n",
      "reached at record 353000\n",
      "reached at record 354000\n",
      "reached at record 355000\n",
      "reached at record 356000\n",
      "reached at record 357000\n",
      "reached at record 358000\n",
      "reached at record 359000\n",
      "reached at record 360000\n",
      "reached at record 361000\n",
      "reached at record 362000\n",
      "reached at record 363000\n",
      "reached at record 364000\n",
      "reached at record 365000\n",
      "reached at record 366000\n",
      "reached at record 367000\n",
      "reached at record 368000\n",
      "reached at record 369000\n",
      "reached at record 370000\n",
      "reached at record 371000\n",
      "reached at record 372000\n",
      "reached at record 373000\n",
      "reached at record 374000\n",
      "reached at record 375000\n",
      "reached at record 376000\n",
      "reached at record 377000\n",
      "reached at record 378000\n",
      "reached at record 379000\n",
      "reached at record 380000\n",
      "reached at record 381000\n",
      "reached at record 382000\n",
      "reached at record 383000\n",
      "reached at record 384000\n",
      "reached at record 385000\n",
      "reached at record 386000\n",
      "reached at record 387000\n",
      "reached at record 388000\n",
      "reached at record 389000\n",
      "reached at record 390000\n",
      "reached at record 391000\n",
      "reached at record 392000\n",
      "reached at record 393000\n",
      "reached at record 394000\n",
      "reached at record 395000\n",
      "reached at record 396000\n",
      "reached at record 397000\n",
      "reached at record 398000\n",
      "reached at record 399000\n",
      "reached at record 400000\n",
      "reached at record 401000\n",
      "reached at record 402000\n",
      "reached at record 403000\n",
      "reached at record 404000\n",
      "reached at record 405000\n",
      "reached at record 406000\n",
      "reached at record 407000\n",
      "reached at record 408000\n",
      "reached at record 409000\n",
      "reached at record 410000\n",
      "reached at record 411000\n",
      "reached at record 412000\n",
      "reached at record 413000\n",
      "reached at record 414000\n",
      "reached at record 415000\n",
      "reached at record 416000\n",
      "reached at record 417000\n",
      "reached at record 418000\n",
      "reached at record 419000\n",
      "reached at record 420000\n",
      "reached at record 421000\n",
      "reached at record 422000\n",
      "reached at record 423000\n",
      "reached at record 424000\n",
      "reached at record 425000\n",
      "reached at record 426000\n",
      "reached at record 427000\n",
      "reached at record 428000\n",
      "reached at record 429000\n",
      "reached at record 430000\n",
      "reached at record 431000\n",
      "reached at record 432000\n",
      "reached at record 433000\n",
      "reached at record 434000\n",
      "reached at record 435000\n",
      "reached at record 436000\n",
      "reached at record 437000\n",
      "reached at record 438000\n",
      "reached at record 439000\n",
      "reached at record 440000\n",
      "reached at record 441000\n",
      "reached at record 442000\n",
      "reached at record 443000\n",
      "reached at record 444000\n",
      "reached at record 445000\n",
      "reached at record 446000\n",
      "reached at record 447000\n",
      "reached at record 448000\n",
      "reached at record 449000\n",
      "reached at record 450000\n",
      "reached at record 451000\n",
      "reached at record 452000\n",
      "reached at record 453000\n",
      "reached at record 454000\n",
      "reached at record 455000\n",
      "reached at record 456000\n",
      "reached at record 457000\n",
      "reached at record 458000\n",
      "reached at record 459000\n",
      "reached at record 460000\n",
      "reached at record 461000\n",
      "reached at record 462000\n",
      "reached at record 463000\n",
      "reached at record 464000\n",
      "reached at record 465000\n",
      "reached at record 466000\n",
      "reached at record 467000\n",
      "reached at record 468000\n",
      "reached at record 469000\n",
      "reached at record 470000\n",
      "reached at record 471000\n",
      "reached at record 472000\n",
      "reached at record 473000\n",
      "reached at record 474000\n",
      "reached at record 475000\n",
      "reached at record 476000\n",
      "reached at record 477000\n",
      "reached at record 478000\n",
      "reached at record 479000\n",
      "reached at record 480000\n",
      "reached at record 481000\n",
      "reached at record 482000\n",
      "reached at record 483000\n",
      "reached at record 484000\n",
      "reached at record 485000\n",
      "reached at record 486000\n",
      "reached at record 487000\n",
      "reached at record 488000\n",
      "reached at record 489000\n",
      "reached at record 490000\n",
      "reached at record 491000\n",
      "reached at record 492000\n",
      "reached at record 493000\n",
      "reached at record 494000\n",
      "reached at record 495000\n",
      "reached at record 496000\n",
      "reached at record 497000\n",
      "reached at record 498000\n",
      "reached at record 499000\n",
      "reached at record 500000\n",
      "reached at record 501000\n",
      "reached at record 502000\n",
      "reached at record 503000\n",
      "reached at record 504000\n",
      "reached at record 505000\n",
      "reached at record 506000\n",
      "reached at record 507000\n",
      "reached at record 508000\n",
      "reached at record 509000\n",
      "reached at record 510000\n",
      "reached at record 511000\n",
      "reached at record 512000\n",
      "reached at record 513000\n",
      "reached at record 514000\n",
      "reached at record 515000\n",
      "reached at record 516000\n",
      "reached at record 517000\n",
      "reached at record 518000\n",
      "reached at record 519000\n",
      "reached at record 520000\n",
      "reached at record 521000\n",
      "reached at record 522000\n",
      "reached at record 523000\n",
      "reached at record 524000\n",
      "reached at record 525000\n",
      "reached at record 526000\n",
      "reached at record 527000\n",
      "reached at record 528000\n",
      "reached at record 529000\n",
      "reached at record 530000\n",
      "reached at record 531000\n",
      "reached at record 532000\n",
      "reached at record 533000\n",
      "reached at record 534000\n",
      "reached at record 535000\n",
      "reached at record 536000\n",
      "reached at record 537000\n",
      "reached at record 538000\n",
      "reached at record 539000\n",
      "reached at record 540000\n",
      "reached at record 541000\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"./train_data.hdf5\", \"w\") as f:\n",
    "    train_data = f.create_dataset(\"train_data\", (X_train.shape[0], 500, 25), \"float16\", **{\"chunks\" : ( 1024, 500, 25)})\n",
    "    for index, data in enumerate(X_train):\n",
    "        for index_2, word in enumerate(data):\n",
    "            if word == \"!\":\n",
    "                vec = np.zeros(shape =(25,))\n",
    "            else :\n",
    "                vec = embeddings_dict.get(word, np.zeros(shape = (25,)))\n",
    "            train_data[index][index_2] = vec\n",
    "        if index % 1000 == 0:\n",
    "            print(f\"reached at record {index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2275ce6-3374-45f9-bab8-e8558809d927",
   "metadata": {},
   "source": [
    "#### D. Build a base model for Supervised Learning - Classification. [2 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a489aab-7e93-41fc-990f-2899e90d23ba",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n",
      "(20000, 500, 25)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gc.collect()\n",
    "\n",
    "with h5py.File(\"train_data.hdf5\", \"r\")  as data:\n",
    "    with h5py.File(\"train_labels.hdf5\", \"r\") as labels:\n",
    "        data_ = data.require_dataset(\"train_data\", shape = (541278, 500, 25), dtype = \"float16\", **{\"chunks\" : ( 64, 500, 25)})\n",
    "        labels_ = labels.require_dataset(\"train_labels\", shape = (541278, 40), dtype = \"int\", **{\"chunks\" : ( 64, 500, 25)})\n",
    "        #print(labels_[ : 50])\n",
    "        clf = OneVsRestClassifier(SGDClassifier(loss = \"hinge\", verbose = 0, warm_start = True), verbose = 0).fit(data_[: 20_000].reshape(20_000, 500*25), labels_[ : 20_000])\n",
    "        next_start = 20_000\n",
    "        while True:\n",
    "            if next_start + 20_000 >= 541278 :\n",
    "                break            \n",
    "            print(data_[next_start : next_start + 20_000].shape)\n",
    "            clf = clf.fit(data_[next_start : next_start + 20_000].reshape(20_000, 500*25), \n",
    "                          labels_[ next_start : next_start + 20_000],\n",
    "                         \n",
    "                         )\n",
    "            next_start = next_start + 20_000\n",
    "            if next_start >= 541278 :\n",
    "                break\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1aa9f3ff-a4d2-46d8-adfa-a800cdbdeaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear_svm_model_uniform_weights.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf, \"linear_svm_model_uniform_weights.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126ca7a-42ef-447e-81a2-19d8b8495983",
   "metadata": {},
   "source": [
    "#### E. Clearly print Performance Metrics. [1 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d54493ff-c8da-4bbc-a513-ea1a0eca8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       122\n",
      "           1       0.00      0.00      0.00       146\n",
      "           2       0.00      0.00      0.00        38\n",
      "           3       0.00      0.00      0.00        48\n",
      "           4       0.00      0.00      0.00       950\n",
      "           5       0.00      0.00      0.00        32\n",
      "           6       0.00      0.00      0.00       119\n",
      "           7       0.00      0.00      0.00        65\n",
      "           8       0.00      0.00      0.00       147\n",
      "           9       0.00      0.00      0.00       112\n",
      "          10       0.00      0.00      0.00       588\n",
      "          11       0.00      0.00      0.00        30\n",
      "          12       0.00      0.00      0.00       158\n",
      "          13       0.00      0.00      0.00       862\n",
      "          14       0.00      0.00      0.00       321\n",
      "          15       0.00      0.00      0.00        10\n",
      "          16       0.00      0.00      0.00       131\n",
      "          17       0.00      0.00      0.00       216\n",
      "          18       0.00      0.00      0.00        75\n",
      "          19       0.00      0.00      0.00       435\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.00      0.00      0.00       280\n",
      "          22       0.00      0.00      0.00        62\n",
      "          23       0.00      0.00      0.00        47\n",
      "          24       0.00      0.00      0.00         8\n",
      "          25       0.00      0.00      0.00       127\n",
      "          26       0.00      0.00      0.00        85\n",
      "          27       0.00      0.00      0.00       104\n",
      "          28       0.00      0.00      0.00       425\n",
      "          29       0.00      0.00      0.00       217\n",
      "          30       0.00      0.00      0.00       100\n",
      "          31       0.00      0.00      0.00       145\n",
      "          32       0.00      0.00      0.00       233\n",
      "          33       0.00      0.00      0.00        80\n",
      "          34       0.00      0.00      0.00      4650\n",
      "          35       0.00      0.00      0.00      1215\n",
      "          36       0.00      0.00      0.00       111\n",
      "          37       0.00      0.00      0.00        51\n",
      "          38       0.00      0.00      0.00        67\n",
      "          39       0.00      0.00      0.00      7352\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     20000\n",
      "   macro avg       0.00      0.00      0.00     20000\n",
      "weighted avg       0.00      0.00      0.00     20000\n",
      " samples avg       0.00      0.00      0.00     20000\n",
      "\n",
      "ROC_AUC score : \t0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with h5py.File(\"./test_data.hdf5\", \"r\") as data:\n",
    "    with h5py.File(\"./test_labels.hdf5\", \"r\") as labels:\n",
    "        data_ = data.require_dataset(\"test_data\", shape = (135320, 500, 25), dtype = \"float16\", **{\"chunks\" : ( 64, 500, 25)})\n",
    "        labels_ = labels.require_dataset(\"test_labels\", shape = (135320, 40), dtype = \"int\", **{\"chunks\" : ( 64, 500, 25)})\n",
    "        preds = clf.predict(data_[ : 20_000].reshape(20_000, 500*25))\n",
    "        #preds_ = ohe.inverse_transform(preds)\n",
    "        #labels_inverse = ohe.inverse_transform(labels_[ : 10_000])\n",
    "        print(classification_report(labels_[ : 20_000], preds))\n",
    "        print(f\"ROC_AUC score : \\t{roc_auc_score(labels_[ : 20_000], preds)}\")\n",
    "        #roc_curve(labels_[ : 20_000].argmax(axis = 1), preds.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9672ae-e15b-4bf0-bb3d-c77588f7b299",
   "metadata": {},
   "source": [
    "How come performance is so bad????????? Aren't semantic vectorizers supposed to be much better than things like Bag-of-Words and tf-idf??? It's not like there's a lack of data for the classifier to learn from..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e357f10-6838-4544-85e9-df358e4f004e",
   "metadata": {},
   "source": [
    "### 4. Improve Performance of model. [14 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddafce3-d23d-4464-8ba4-d896837485cb",
   "metadata": {},
   "source": [
    "#### A. Experiment with other vectorisers. [4 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83230290-ddff-4754-a083-284480a63be4",
   "metadata": {},
   "source": [
    "Now I'll try normal BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9066409-60b4-4efa-a101-add362022f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range = (1, 2), \n",
    "                             binary = True\n",
    "                            )\n",
    "# restricting to 20_000 because I don't have the time and compute resources to do the hdf5 thing again, and HDF5 doesn't work for `object` arrays anyways\n",
    "\n",
    "vectorizer = vectorizer.fit(X_train)\n",
    "X_train_bow = vectorizer.transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21510bee-d299-4578-8a19-a249a79a4bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((541278, 10162869), (135320, 10162869))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.shape, X_test_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3eb477-98aa-43a8-9be2-59f37ee40691",
   "metadata": {},
   "source": [
    "Oh there is also something called `zarr` that supports `object` dtype... let's see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43fec3c1-a49d-4e0c-83c3-f567253e46bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import numcodecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "efaef928-b6f5-489f-a195-b8ad46f99140",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "astype() got an unexpected keyword argument 'order'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_bow \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train_bow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_codec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnumcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_bow \u001b[38;5;241m=\u001b[39m zarr\u001b[38;5;241m.\u001b[39marray( data \u001b[38;5;241m=\u001b[39m X_test_bow, dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m, object_codec \u001b[38;5;241m=\u001b[39m numcodecs\u001b[38;5;241m.\u001b[39mPickle())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\zarr\\creation.py:380\u001b[0m, in \u001b[0;36marray\u001b[1;34m(data, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m z \u001b[38;5;241m=\u001b[39m create(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# fill with data\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m z[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# set read_only property afterwards\u001b[39;00m\n\u001b[0;32m    383\u001b[0m z\u001b[38;5;241m.\u001b[39mread_only \u001b[38;5;241m=\u001b[39m read_only\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\zarr\\core.py:1353\u001b[0m, in \u001b[0;36mArray.__setitem__\u001b[1;34m(self, selection, value)\u001b[0m\n\u001b[0;32m   1351\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvindex[selection] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_basic_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpure_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\zarr\\core.py:1448\u001b[0m, in \u001b[0;36mArray.set_basic_selection\u001b[1;34m(self, selection, value, fields)\u001b[0m\n\u001b[0;32m   1446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_basic_selection_zd(selection, value, fields\u001b[38;5;241m=\u001b[39mfields)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_basic_selection_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\zarr\\core.py:1748\u001b[0m, in \u001b[0;36mArray._set_basic_selection_nd\u001b[1;34m(self, selection, value, fields)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_basic_selection_nd\u001b[39m(\u001b[38;5;28mself\u001b[39m, selection, value, fields\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# implementation of __setitem__ for array with at least one dimension\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;66;03m# setup indexer\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m BasicIndexer(selection, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1748\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\zarr\\core.py:1800\u001b[0m, in \u001b[0;36mArray._set_selection\u001b[1;34m(self, indexer, value, fields)\u001b[0m\n\u001b[0;32m   1797\u001b[0m                 chunk_value \u001b[38;5;241m=\u001b[39m chunk_value[item]\n\u001b[0;32m   1799\u001b[0m         \u001b[38;5;66;03m# put data\u001b[39;00m\n\u001b[1;32m-> 1800\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunk_setitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1802\u001b[0m     lchunk_coords, lchunk_selection, lout_selection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mindexer)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\zarr\\core.py:2062\u001b[0m, in \u001b[0;36mArray._chunk_setitem\u001b[1;34m(self, chunk_coords, chunk_selection, value, fields)\u001b[0m\n\u001b[0;32m   2059\u001b[0m     lock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_synchronizer[ckey]\n\u001b[0;32m   2061\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[1;32m-> 2062\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunk_setitem_nosync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2063\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\zarr\\core.py:2067\u001b[0m, in \u001b[0;36mArray._chunk_setitem_nosync\u001b[1;34m(self, chunk_coords, chunk_selection, value, fields)\u001b[0m\n\u001b[0;32m   2065\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chunk_setitem_nosync\u001b[39m(\u001b[38;5;28mself\u001b[39m, chunk_coords, chunk_selection, value, fields\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2066\u001b[0m     ckey \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_key(chunk_coords)\n\u001b[1;32m-> 2067\u001b[0m     cdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_for_setitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;66;03m# attempt to delete chunk if it only contains the fill value\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_empty_chunks) \u001b[38;5;129;01mand\u001b[39;00m all_equal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value, cdata):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\zarr\\core.py:2091\u001b[0m, in \u001b[0;36mArray._process_for_setitem\u001b[1;34m(self, ckey, chunk_selection, value, fields)\u001b[0m\n\u001b[0;32m   2086\u001b[0m         chunk\u001b[38;5;241m.\u001b[39mfill(value)\n\u001b[0;32m   2088\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2089\u001b[0m \n\u001b[0;32m   2090\u001b[0m         \u001b[38;5;66;03m# ensure array is contiguous\u001b[39;00m\n\u001b[1;32m-> 2091\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2094\u001b[0m     \u001b[38;5;66;03m# partially replace the contents of this chunk\u001b[39;00m\n\u001b[0;32m   2096\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2097\u001b[0m \n\u001b[0;32m   2098\u001b[0m         \u001b[38;5;66;03m# obtain compressed data for chunk\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: astype() got an unexpected keyword argument 'order'"
     ]
    }
   ],
   "source": [
    "train_bow = zarr.array( data = X_train_bow, dtype = \"object\", object_codec = numcodecs.Pickle())\n",
    "\n",
    "test_bow = zarr.array( data = X_test_bow, dtype = \"object\", object_codec = numcodecs.Pickle())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0f6f6-9de4-4829-919d-50956940cb21",
   "metadata": {},
   "source": [
    "It's also not working... Google brings up nothing, except possibly a versioning issue...maybe I'll take a deeper look later\n",
    "\n",
    "Now I just subset the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27dd965b-cb01-45be-96f6-85e7a925f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = OneVsRestClassifier(SGDClassifier(loss = \"log_loss\")).fit(X_train_bow[ : 20_000], train_labels[ : 20_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d94b6cf8-f4d5-4753-81c9-2925202a8e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       122\n",
      "           1       0.00      0.00      0.00       146\n",
      "           2       0.00      0.00      0.00        38\n",
      "           3       0.00      0.00      0.00        48\n",
      "           4       0.08      0.00      0.00       950\n",
      "           5       0.00      0.00      0.00        32\n",
      "           6       0.50      0.01      0.02       119\n",
      "           7       0.00      0.00      0.00        65\n",
      "           8       0.00      0.00      0.00       147\n",
      "           9       0.00      0.00      0.00       112\n",
      "          10       0.50      0.00      0.00       588\n",
      "          11       0.00      0.00      0.00        30\n",
      "          12       0.00      0.00      0.00       158\n",
      "          13       0.22      0.00      0.00       862\n",
      "          14       0.00      0.00      0.00       321\n",
      "          15       0.00      0.00      0.00        10\n",
      "          16       0.00      0.00      0.00       131\n",
      "          17       0.00      0.00      0.00       216\n",
      "          18       0.00      0.00      0.00        75\n",
      "          19       0.17      0.00      0.00       435\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.50      0.00      0.01       280\n",
      "          22       0.00      0.00      0.00        62\n",
      "          23       0.00      0.00      0.00        47\n",
      "          24       0.00      0.00      0.00         8\n",
      "          25       0.00      0.00      0.00       127\n",
      "          26       0.00      0.00      0.00        85\n",
      "          27       0.00      0.00      0.00       104\n",
      "          28       0.36      0.02      0.04       425\n",
      "          29       0.00      0.00      0.00       217\n",
      "          30       0.00      0.00      0.00       100\n",
      "          31       0.00      0.00      0.00       145\n",
      "          32       0.00      0.00      0.00       233\n",
      "          33       0.00      0.00      0.00        80\n",
      "          34       0.48      0.15      0.22      4650\n",
      "          35       0.50      0.01      0.03      1215\n",
      "          36       0.00      0.00      0.00       111\n",
      "          37       0.00      0.00      0.00        51\n",
      "          38       1.00      0.04      0.09        67\n",
      "          39       0.43      0.23      0.30      7352\n",
      "\n",
      "   micro avg       0.44      0.12      0.19     20000\n",
      "   macro avg       0.12      0.01      0.02     20000\n",
      "weighted avg       0.35      0.12      0.16     20000\n",
      " samples avg       0.12      0.12      0.12     20000\n",
      "\n",
      "ROC_AUC score :\t0.5030404374607343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "log_reg_preds = log_reg.predict(X_test_bow[ : 20_000])\n",
    "print(classification_report(test_labels[ : 20_000], log_reg_preds))\n",
    "print(f\"ROC_AUC score :\\t{roc_auc_score(test_labels[ : 20_000], log_reg_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb7352-2561-4043-8f65-bb5e5a08b998",
   "metadata": {},
   "source": [
    "This is also very bad... Last effort : `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0232fb-2436-4bd1-bbaa-1aa4d65a04b2",
   "metadata": {},
   "source": [
    "#### B. Build classifier Models using other algorithms than base model. [4 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9bf2274-3dd9-48ed-a49a-316ab1af2f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2311"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train_bow, X_test_bow, log_reg, log_reg_preds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59985270-3e41-446a-a3d8-d0be22186c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer( \n",
    "                         binary = True).fit(X_train[ : -100_000]) # getting memory/logic errors if I go beyond,  \n",
    "                                                                  #or use ngram_range = (1,2) one of the samples is ending up with  \n",
    "                                                                  #__0__ features inside it, causing tfidf to fail\n",
    "X_train_tfidf = tf_idf.transform(X_train[ : -100_000])\n",
    "X_test_tfidf = tf_idf.transform(X_test[ : -100_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2365b9a3-252a-40c8-9820-c1ad8e13d432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_reg_tfidf = OneVsRestClassifier(\n",
    "    LogisticRegression(\n",
    "        )\n",
    "    ).fit(X_train_tfidf, train_labels[ : -100_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e252805-eb11-4249-8281-bdb375f801c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.03      0.06       197\n",
      "           1       0.00      0.00      0.00       241\n",
      "           2       0.00      0.00      0.00        65\n",
      "           3       0.00      0.00      0.00        85\n",
      "           4       0.92      0.02      0.04      1679\n",
      "           5       0.00      0.00      0.00        66\n",
      "           6       0.00      0.00      0.00       225\n",
      "           7       0.00      0.00      0.00       120\n",
      "           8       1.00      0.00      0.01       267\n",
      "           9       0.00      0.00      0.00       205\n",
      "          10       0.90      0.01      0.02      1031\n",
      "          11       1.00      0.02      0.04        48\n",
      "          12       1.00      0.00      0.01       283\n",
      "          13       0.82      0.02      0.04      1523\n",
      "          14       0.75      0.02      0.04       571\n",
      "          15       0.00      0.00      0.00        21\n",
      "          16       1.00      0.08      0.14       246\n",
      "          17       0.92      0.03      0.06       368\n",
      "          18       1.00      0.01      0.01       155\n",
      "          19       0.33      0.00      0.00       798\n",
      "          20       0.00      0.00      0.00        75\n",
      "          21       0.33      0.01      0.01       488\n",
      "          22       0.00      0.00      0.00       108\n",
      "          23       0.00      0.00      0.00        99\n",
      "          24       0.00      0.00      0.00        15\n",
      "          25       0.00      0.00      0.00       248\n",
      "          26       0.00      0.00      0.00       152\n",
      "          27       0.67      0.02      0.04       174\n",
      "          28       0.52      0.02      0.03       746\n",
      "          29       1.00      0.02      0.05       381\n",
      "          30       0.00      0.00      0.00       171\n",
      "          31       0.00      0.00      0.00       270\n",
      "          32       0.50      0.00      0.01       396\n",
      "          33       0.00      0.00      0.00       152\n",
      "          34       0.61      0.20      0.30      8074\n",
      "          35       0.76      0.06      0.11      2155\n",
      "          36       1.00      0.01      0.01       198\n",
      "          37       0.00      0.00      0.00        90\n",
      "          38       1.00      0.09      0.16       112\n",
      "          39       0.60      0.21      0.31     13022\n",
      "\n",
      "   micro avg       0.61      0.13      0.22     35320\n",
      "   macro avg       0.43      0.02      0.04     35320\n",
      "weighted avg       0.62      0.13      0.20     35320\n",
      " samples avg       0.13      0.13      0.13     35320\n",
      "\n",
      "ROC_AUC score :\t0.5094201793650431\n"
     ]
    }
   ],
   "source": [
    "log_reg_preds_tfidf = log_reg_tfidf.predict(X_test_tfidf)\n",
    "print(classification_report(test_labels[ : -100_000], log_reg_preds_tfidf))\n",
    "print(f\"ROC_AUC score :\\t{roc_auc_score(test_labels[ : -100_000], log_reg_preds_tfidf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a368c-45f6-4126-83fb-635cf984115d",
   "metadata": {},
   "source": [
    "Nothing happened  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b4ffb-97b7-46db-a3c7-97f97a20186d",
   "metadata": {},
   "source": [
    "### C. Tune Parameters/Hyperparameters of the model/s. [4 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1433a671-03e5-4f03-9e60-208ecfc0488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = {\n",
    "    \"penalty\" :         [\"l2\", \"elasticnet\"],\n",
    "    \"C\" :               [ 1.0, 1e-1, 5e-1, 1e-2, 5e-2, 1e-3, 5e-3],\n",
    "    \"class_weight\" :    [\"balanced\", None],\n",
    "    \"solver\" :          [\"newton-cg\", \"sag\", \"saga\"],\n",
    "    \"l1_ratio\" :        [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f640b1-4015-4741-811d-8214dc036d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "[CV 1/5; 1/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=l2, solver=newton-cg;, score=0.013 total time= 6.4min\n",
      "[CV 2/5; 1/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=l2, solver=newton-cg;, score=0.013 total time= 6.1min\n",
      "[CV 3/5; 1/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=l2, solver=newton-cg;, score=0.013 total time= 6.6min\n",
      "[CV 4/5; 1/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=l2, solver=newton-cg;, score=0.013 total time=15.4min\n",
      "[CV 5/5; 1/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=l2, solver=newton-cg;, score=0.013 total time= 7.5min\n",
      "[CV 1/5; 2/50] START C=0.005, class_weight=None, l1_ratio=0.9, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 2/50] END C=0.005, class_weight=None, l1_ratio=0.9, penalty=l2, solver=saga;, score=0.015 total time= 2.1min\n",
      "[CV 2/5; 2/50] START C=0.005, class_weight=None, l1_ratio=0.9, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 2/50] END C=0.005, class_weight=None, l1_ratio=0.9, penalty=l2, solver=saga;, score=0.015 total time= 2.0min\n",
      "[CV 3/5; 2/50] START C=0.005, class_weight=None, l1_ratio=0.9, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 2/50] END C=0.005, class_weight=None, l1_ratio=0.9, penalty=l2, solver=saga;, score=0.015 total time= 2.4min\n",
      "[CV 4/5; 2/50] START C=0.005, class_weight=None, l1_ratio=0.9, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 2/50] END C=0.005, class_weight=None, l1_ratio=0.9, penalty=l2, solver=saga;, score=0.015 total time= 1.9min\n",
      "[CV 5/5; 2/50] START C=0.005, class_weight=None, l1_ratio=0.9, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 2/50] END C=0.005, class_weight=None, l1_ratio=0.9, penalty=l2, solver=saga;, score=0.015 total time= 2.1min\n",
      "[CV 1/5; 3/50] START C=0.01, class_weight=balanced, l1_ratio=0.6, penalty=elasticnet, solver=newton-cg\n",
      "[CV 1/5; 3/50] END C=0.01, class_weight=balanced, l1_ratio=0.6, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5; 3/50] START C=0.01, class_weight=balanced, l1_ratio=0.6, penalty=elasticnet, solver=newton-cg\n",
      "[CV 2/5; 3/50] END C=0.01, class_weight=balanced, l1_ratio=0.6, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5; 3/50] START C=0.01, class_weight=balanced, l1_ratio=0.6, penalty=elasticnet, solver=newton-cg\n",
      "[CV 3/5; 3/50] END C=0.01, class_weight=balanced, l1_ratio=0.6, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5; 3/50] START C=0.01, class_weight=balanced, l1_ratio=0.6, penalty=elasticnet, solver=newton-cg\n",
      "[CV 4/5; 3/50] END C=0.01, class_weight=balanced, l1_ratio=0.6, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5; 3/50] START C=0.01, class_weight=balanced, l1_ratio=0.6, penalty=elasticnet, solver=newton-cg\n",
      "[CV 5/5; 3/50] END C=0.01, class_weight=balanced, l1_ratio=0.6, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5; 4/50] START C=0.01, class_weight=balanced, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg\n",
      "[CV 1/5; 4/50] END C=0.01, class_weight=balanced, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5; 4/50] START C=0.01, class_weight=balanced, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg\n",
      "[CV 2/5; 4/50] END C=0.01, class_weight=balanced, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5; 4/50] START C=0.01, class_weight=balanced, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg\n",
      "[CV 3/5; 4/50] END C=0.01, class_weight=balanced, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5; 4/50] START C=0.01, class_weight=balanced, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg\n",
      "[CV 4/5; 4/50] END C=0.01, class_weight=balanced, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5; 4/50] START C=0.01, class_weight=balanced, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg\n",
      "[CV 5/5; 4/50] END C=0.01, class_weight=balanced, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5; 5/50] START C=0.001, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 5/50] END C=0.001, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=sag;, score=0.060 total time= 1.8min\n",
      "[CV 2/5; 5/50] START C=0.001, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/50] END C=0.001, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=sag;, score=0.060 total time= 1.8min\n",
      "[CV 3/5; 5/50] START C=0.001, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 5/50] END C=0.001, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=sag;, score=0.060 total time= 1.7min\n",
      "[CV 4/5; 5/50] START C=0.001, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 5/50] END C=0.001, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=sag;, score=0.057 total time= 1.5min\n",
      "[CV 5/5; 5/50] START C=0.001, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 5/50] END C=0.001, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=sag;, score=0.058 total time= 1.6min\n",
      "[CV 1/5; 6/50] START C=0.1, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 6/50] END C=0.1, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.085 total time= 6.6min\n",
      "[CV 2/5; 6/50] START C=0.1, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/50] END C=0.1, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.087 total time= 6.6min\n",
      "[CV 3/5; 6/50] START C=0.1, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 6/50] END C=0.1, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.088 total time= 6.6min\n",
      "[CV 4/5; 6/50] START C=0.1, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 6/50] END C=0.1, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.085 total time= 6.6min\n",
      "[CV 5/5; 6/50] START C=0.1, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 6/50] END C=0.1, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.086 total time= 6.6min\n",
      "[CV 1/5; 7/50] START C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 7/50] END C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.078 total time= 1.9min\n",
      "[CV 2/5; 7/50] START C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 7/50] END C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.079 total time= 1.9min\n",
      "[CV 3/5; 7/50] START C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 7/50] END C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.081 total time= 1.7min\n",
      "[CV 4/5; 7/50] START C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 7/50] END C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.078 total time= 1.9min\n",
      "[CV 5/5; 7/50] START C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 7/50] END C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.078 total time= 1.9min\n",
      "[CV 1/5; 8/50] START C=0.05, class_weight=None, l1_ratio=0.1, penalty=elasticnet, solver=sag\n",
      "[CV 1/5; 8/50] END C=0.05, class_weight=None, l1_ratio=0.1, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5; 8/50] START C=0.05, class_weight=None, l1_ratio=0.1, penalty=elasticnet, solver=sag\n",
      "[CV 2/5; 8/50] END C=0.05, class_weight=None, l1_ratio=0.1, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5; 8/50] START C=0.05, class_weight=None, l1_ratio=0.1, penalty=elasticnet, solver=sag\n",
      "[CV 3/5; 8/50] END C=0.05, class_weight=None, l1_ratio=0.1, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5; 8/50] START C=0.05, class_weight=None, l1_ratio=0.1, penalty=elasticnet, solver=sag\n",
      "[CV 4/5; 8/50] END C=0.05, class_weight=None, l1_ratio=0.1, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5; 8/50] START C=0.05, class_weight=None, l1_ratio=0.1, penalty=elasticnet, solver=sag\n",
      "[CV 5/5; 8/50] END C=0.05, class_weight=None, l1_ratio=0.1, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5; 9/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga\n",
      "[CV 1/5; 9/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga;, score=0.013 total time=  58.4s\n",
      "[CV 2/5; 9/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga\n",
      "[CV 2/5; 9/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga;, score=0.013 total time= 1.2min\n",
      "[CV 3/5; 9/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga\n",
      "[CV 3/5; 9/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga;, score=0.013 total time= 1.1min\n",
      "[CV 4/5; 9/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga\n",
      "[CV 4/5; 9/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga;, score=0.013 total time= 1.1min\n",
      "[CV 5/5; 9/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga\n",
      "[CV 5/5; 9/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga;, score=0.013 total time= 1.1min\n",
      "[CV 1/5; 10/50] START C=0.05, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 10/50] END C=0.05, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=saga;, score=0.078 total time= 8.2min\n",
      "[CV 2/5; 10/50] START C=0.05, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 10/50] END C=0.05, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=saga;, score=0.079 total time= 8.2min\n",
      "[CV 3/5; 10/50] START C=0.05, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 10/50] END C=0.05, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=saga;, score=0.080 total time= 8.2min\n",
      "[CV 4/5; 10/50] START C=0.05, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 10/50] END C=0.05, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=saga;, score=0.078 total time=13.3min\n",
      "[CV 5/5; 10/50] START C=0.05, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 10/50] END C=0.05, class_weight=balanced, l1_ratio=0.1, penalty=l2, solver=saga;, score=0.078 total time= 7.2min\n",
      "[CV 1/5; 11/50] START C=0.05, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 11/50] END C=0.05, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga;, score=0.022 total time= 1.6min\n",
      "[CV 2/5; 11/50] START C=0.05, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 11/50] END C=0.05, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga;, score=0.023 total time= 1.6min\n",
      "[CV 3/5; 11/50] START C=0.05, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 11/50] END C=0.05, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga;, score=0.022 total time= 1.7min\n",
      "[CV 4/5; 11/50] START C=0.05, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 11/50] END C=0.05, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga;, score=0.022 total time= 1.7min\n",
      "[CV 5/5; 11/50] START C=0.05, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 11/50] END C=0.05, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga;, score=0.022 total time= 1.6min\n",
      "[CV 1/5; 12/50] START C=1.0, class_weight=balanced, l1_ratio=0.6, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 12/50] END C=1.0, class_weight=balanced, l1_ratio=0.6, penalty=l2, solver=saga;, score=0.107 total time= 6.9min\n",
      "[CV 2/5; 12/50] START C=1.0, class_weight=balanced, l1_ratio=0.6, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 12/50] END C=1.0, class_weight=balanced, l1_ratio=0.6, penalty=l2, solver=saga;, score=0.116 total time= 7.3min\n",
      "[CV 3/5; 12/50] START C=1.0, class_weight=balanced, l1_ratio=0.6, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 12/50] END C=1.0, class_weight=balanced, l1_ratio=0.6, penalty=l2, solver=saga;, score=0.108 total time= 7.3min\n",
      "[CV 4/5; 12/50] START C=1.0, class_weight=balanced, l1_ratio=0.6, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 12/50] END C=1.0, class_weight=balanced, l1_ratio=0.6, penalty=l2, solver=saga;, score=0.109 total time= 6.9min\n",
      "[CV 5/5; 12/50] START C=1.0, class_weight=balanced, l1_ratio=0.6, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 12/50] END C=1.0, class_weight=balanced, l1_ratio=0.6, penalty=l2, solver=saga;, score=0.119 total time= 6.9min\n",
      "[CV 1/5; 13/50] START C=0.1, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga\n",
      "[CV 1/5; 13/50] END C=0.1, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga;, score=0.029 total time= 2.8min\n",
      "[CV 2/5; 13/50] START C=0.1, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga\n",
      "[CV 2/5; 13/50] END C=0.1, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga;, score=0.027 total time= 2.6min\n",
      "[CV 3/5; 13/50] START C=0.1, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga\n",
      "[CV 3/5; 13/50] END C=0.1, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga;, score=0.030 total time= 2.6min\n",
      "[CV 4/5; 13/50] START C=0.1, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga\n",
      "[CV 4/5; 13/50] END C=0.1, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga;, score=0.029 total time= 2.7min\n",
      "[CV 5/5; 13/50] START C=0.1, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga\n",
      "[CV 5/5; 13/50] END C=0.1, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=saga;, score=0.029 total time= 2.9min\n",
      "[CV 1/5; 14/50] START C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 14/50] END C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=sag;, score=0.085 total time= 5.6min\n",
      "[CV 2/5; 14/50] START C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 14/50] END C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=sag;, score=0.087 total time= 5.5min\n",
      "[CV 3/5; 14/50] START C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 14/50] END C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=sag;, score=0.088 total time= 5.4min\n",
      "[CV 4/5; 14/50] START C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 14/50] END C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=sag;, score=0.085 total time= 5.4min\n",
      "[CV 5/5; 14/50] START C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 14/50] END C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=sag;, score=0.087 total time= 5.5min\n",
      "[CV 1/5; 15/50] START C=0.005, class_weight=None, l1_ratio=0.8, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 15/50] END C=0.005, class_weight=None, l1_ratio=0.8, penalty=l2, solver=sag;, score=0.015 total time=  59.0s\n",
      "[CV 2/5; 15/50] START C=0.005, class_weight=None, l1_ratio=0.8, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 15/50] END C=0.005, class_weight=None, l1_ratio=0.8, penalty=l2, solver=sag;, score=0.015 total time=  55.3s\n",
      "[CV 3/5; 15/50] START C=0.005, class_weight=None, l1_ratio=0.8, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 15/50] END C=0.005, class_weight=None, l1_ratio=0.8, penalty=l2, solver=sag;, score=0.015 total time=  48.2s\n",
      "[CV 4/5; 15/50] START C=0.005, class_weight=None, l1_ratio=0.8, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 15/50] END C=0.005, class_weight=None, l1_ratio=0.8, penalty=l2, solver=sag;, score=0.015 total time=  48.0s\n",
      "[CV 5/5; 15/50] START C=0.005, class_weight=None, l1_ratio=0.8, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 15/50] END C=0.005, class_weight=None, l1_ratio=0.8, penalty=l2, solver=sag;, score=0.015 total time=  55.1s\n",
      "[CV 1/5; 16/50] START C=1.0, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 16/50] END C=1.0, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg;, score=0.113 total time=21.2min\n",
      "[CV 2/5; 16/50] START C=1.0, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 16/50] END C=1.0, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg;, score=0.113 total time=21.6min\n",
      "[CV 3/5; 16/50] START C=1.0, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 16/50] END C=1.0, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg;, score=0.116 total time=21.8min\n",
      "[CV 4/5; 16/50] START C=1.0, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 16/50] END C=1.0, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg;, score=0.112 total time=23.7min\n",
      "[CV 5/5; 16/50] START C=1.0, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 16/50] END C=1.0, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg;, score=0.114 total time=21.8min\n",
      "[CV 1/5; 17/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=sag\n",
      "[CV 1/5; 17/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5; 17/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=sag\n",
      "[CV 2/5; 17/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5; 17/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=sag\n",
      "[CV 3/5; 17/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5; 17/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=sag\n",
      "[CV 4/5; 17/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5; 17/50] START C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=sag\n",
      "[CV 5/5; 17/50] END C=0.001, class_weight=None, l1_ratio=0.8, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5; 18/50] START C=0.001, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg\n",
      "[CV 1/5; 18/50] END C=0.001, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5; 18/50] START C=0.001, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg\n",
      "[CV 2/5; 18/50] END C=0.001, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5; 18/50] START C=0.001, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg\n",
      "[CV 3/5; 18/50] END C=0.001, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5; 18/50] START C=0.001, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg\n",
      "[CV 4/5; 18/50] END C=0.001, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5; 18/50] START C=0.001, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg\n",
      "[CV 5/5; 18/50] END C=0.001, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5; 19/50] START C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg\n",
      "[CV 1/5; 19/50] END C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5; 19/50] START C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg\n",
      "[CV 2/5; 19/50] END C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5; 19/50] START C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg\n",
      "[CV 3/5; 19/50] END C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5; 19/50] START C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg\n",
      "[CV 4/5; 19/50] END C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5; 19/50] START C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg\n",
      "[CV 5/5; 19/50] END C=0.05, class_weight=balanced, l1_ratio=0.5, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5; 20/50] START C=0.001, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga\n",
      "[CV 1/5; 20/50] END C=0.001, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga;, score=0.013 total time=  50.5s\n",
      "[CV 2/5; 20/50] START C=0.001, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga\n",
      "[CV 2/5; 20/50] END C=0.001, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga;, score=0.013 total time=  56.4s\n",
      "[CV 3/5; 20/50] START C=0.001, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga\n",
      "[CV 3/5; 20/50] END C=0.001, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga;, score=0.013 total time=  50.6s\n",
      "[CV 4/5; 20/50] START C=0.001, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga\n",
      "[CV 4/5; 20/50] END C=0.001, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga;, score=0.013 total time=  56.0s\n",
      "[CV 5/5; 20/50] START C=0.001, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga\n",
      "[CV 5/5; 20/50] END C=0.001, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga;, score=0.013 total time=  50.5s\n",
      "[CV 1/5; 21/50] START C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 21/50] END C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=newton-cg;, score=0.085 total time= 7.3min\n",
      "[CV 2/5; 21/50] START C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 21/50] END C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=newton-cg;, score=0.086 total time= 7.2min\n",
      "[CV 3/5; 21/50] START C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 21/50] END C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=newton-cg;, score=0.088 total time= 6.7min\n",
      "[CV 4/5; 21/50] START C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 21/50] END C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=newton-cg;, score=0.085 total time= 6.4min\n",
      "[CV 5/5; 21/50] START C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 21/50] END C=0.1, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=newton-cg;, score=0.086 total time= 6.4min\n",
      "[CV 1/5; 22/50] START C=0.001, class_weight=balanced, l1_ratio=0.8, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 22/50] END C=0.001, class_weight=balanced, l1_ratio=0.8, penalty=l2, solver=sag;, score=0.060 total time= 1.6min\n",
      "[CV 2/5; 22/50] START C=0.001, class_weight=balanced, l1_ratio=0.8, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 22/50] END C=0.001, class_weight=balanced, l1_ratio=0.8, penalty=l2, solver=sag;, score=0.060 total time= 1.6min\n",
      "[CV 3/5; 22/50] START C=0.001, class_weight=balanced, l1_ratio=0.8, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 22/50] END C=0.001, class_weight=balanced, l1_ratio=0.8, penalty=l2, solver=sag;, score=0.060 total time= 1.5min\n",
      "[CV 4/5; 22/50] START C=0.001, class_weight=balanced, l1_ratio=0.8, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 22/50] END C=0.001, class_weight=balanced, l1_ratio=0.8, penalty=l2, solver=sag;, score=0.058 total time= 1.5min\n",
      "[CV 5/5; 22/50] START C=0.001, class_weight=balanced, l1_ratio=0.8, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 22/50] END C=0.001, class_weight=balanced, l1_ratio=0.8, penalty=l2, solver=sag;, score=0.058 total time= 1.5min\n",
      "[CV 1/5; 23/50] START C=0.01, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg\n",
      "[CV 1/5; 23/50] END C=0.01, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 2/5; 23/50] START C=0.01, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg\n",
      "[CV 2/5; 23/50] END C=0.01, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 3/5; 23/50] START C=0.01, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg\n",
      "[CV 3/5; 23/50] END C=0.01, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 4/5; 23/50] START C=0.01, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg\n",
      "[CV 4/5; 23/50] END C=0.01, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 5/5; 23/50] START C=0.01, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg\n",
      "[CV 5/5; 23/50] END C=0.01, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n",
      "[CV 1/5; 24/50] START C=0.01, class_weight=None, l1_ratio=0.5, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 24/50] END C=0.01, class_weight=None, l1_ratio=0.5, penalty=l2, solver=saga;, score=0.018 total time= 1.6min\n",
      "[CV 2/5; 24/50] START C=0.01, class_weight=None, l1_ratio=0.5, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 24/50] END C=0.01, class_weight=None, l1_ratio=0.5, penalty=l2, solver=saga;, score=0.018 total time= 1.6min\n",
      "[CV 3/5; 24/50] START C=0.01, class_weight=None, l1_ratio=0.5, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 24/50] END C=0.01, class_weight=None, l1_ratio=0.5, penalty=l2, solver=saga;, score=0.018 total time= 1.7min\n",
      "[CV 4/5; 24/50] START C=0.01, class_weight=None, l1_ratio=0.5, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 24/50] END C=0.01, class_weight=None, l1_ratio=0.5, penalty=l2, solver=saga;, score=0.018 total time= 1.7min\n",
      "[CV 5/5; 24/50] START C=0.01, class_weight=None, l1_ratio=0.5, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 24/50] END C=0.01, class_weight=None, l1_ratio=0.5, penalty=l2, solver=saga;, score=0.018 total time= 1.6min\n",
      "[CV 1/5; 25/50] START C=0.005, class_weight=None, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 25/50] END C=0.005, class_weight=None, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.015 total time=  52.0s\n",
      "[CV 2/5; 25/50] START C=0.005, class_weight=None, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 25/50] END C=0.005, class_weight=None, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.015 total time=  59.5s\n",
      "[CV 3/5; 25/50] START C=0.005, class_weight=None, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 25/50] END C=0.005, class_weight=None, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.015 total time=  48.4s\n",
      "[CV 4/5; 25/50] START C=0.005, class_weight=None, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 25/50] END C=0.005, class_weight=None, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.015 total time=  56.0s\n",
      "[CV 5/5; 25/50] START C=0.005, class_weight=None, l1_ratio=0.5, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 25/50] END C=0.005, class_weight=None, l1_ratio=0.5, penalty=l2, solver=sag;, score=0.015 total time=  52.3s\n",
      "[CV 1/5; 26/50] START C=0.005, class_weight=None, l1_ratio=0.2, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 26/50] END C=0.005, class_weight=None, l1_ratio=0.2, penalty=l2, solver=newton-cg;, score=0.015 total time= 7.8min\n",
      "[CV 2/5; 26/50] START C=0.005, class_weight=None, l1_ratio=0.2, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 26/50] END C=0.005, class_weight=None, l1_ratio=0.2, penalty=l2, solver=newton-cg;, score=0.015 total time=12.2min\n",
      "[CV 3/5; 26/50] START C=0.005, class_weight=None, l1_ratio=0.2, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 26/50] END C=0.005, class_weight=None, l1_ratio=0.2, penalty=l2, solver=newton-cg;, score=0.015 total time= 9.1min\n",
      "[CV 4/5; 26/50] START C=0.005, class_weight=None, l1_ratio=0.2, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 26/50] END C=0.005, class_weight=None, l1_ratio=0.2, penalty=l2, solver=newton-cg;, score=0.015 total time= 9.2min\n",
      "[CV 5/5; 26/50] START C=0.005, class_weight=None, l1_ratio=0.2, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 26/50] END C=0.005, class_weight=None, l1_ratio=0.2, penalty=l2, solver=newton-cg;, score=0.015 total time= 9.1min\n",
      "[CV 1/5; 27/50] START C=0.5, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 27/50] END C=0.5, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga;, score=0.049 total time= 2.0min\n",
      "[CV 2/5; 27/50] START C=0.5, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 27/50] END C=0.5, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga;, score=0.050 total time= 2.2min\n",
      "[CV 3/5; 27/50] START C=0.5, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 27/50] END C=0.5, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga;, score=0.051 total time= 2.0min\n",
      "[CV 4/5; 27/50] START C=0.5, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 27/50] END C=0.5, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga;, score=0.050 total time= 1.9min\n",
      "[CV 5/5; 27/50] START C=0.5, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 27/50] END C=0.5, class_weight=None, l1_ratio=0.4, penalty=l2, solver=saga;, score=0.050 total time= 2.0min\n",
      "[CV 1/5; 28/50] START C=0.1, class_weight=balanced, l1_ratio=0.4, penalty=elasticnet, solver=sag\n",
      "[CV 1/5; 28/50] END C=0.1, class_weight=balanced, l1_ratio=0.4, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5; 28/50] START C=0.1, class_weight=balanced, l1_ratio=0.4, penalty=elasticnet, solver=sag\n",
      "[CV 2/5; 28/50] END C=0.1, class_weight=balanced, l1_ratio=0.4, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5; 28/50] START C=0.1, class_weight=balanced, l1_ratio=0.4, penalty=elasticnet, solver=sag\n",
      "[CV 3/5; 28/50] END C=0.1, class_weight=balanced, l1_ratio=0.4, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5; 28/50] START C=0.1, class_weight=balanced, l1_ratio=0.4, penalty=elasticnet, solver=sag\n",
      "[CV 4/5; 28/50] END C=0.1, class_weight=balanced, l1_ratio=0.4, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5; 28/50] START C=0.1, class_weight=balanced, l1_ratio=0.4, penalty=elasticnet, solver=sag\n",
      "[CV 5/5; 28/50] END C=0.1, class_weight=balanced, l1_ratio=0.4, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5; 29/50] START C=0.001, class_weight=None, l1_ratio=0.2, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 29/50] END C=0.001, class_weight=None, l1_ratio=0.2, penalty=l2, solver=sag;, score=0.013 total time= 1.4min\n",
      "[CV 2/5; 29/50] START C=0.001, class_weight=None, l1_ratio=0.2, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 29/50] END C=0.001, class_weight=None, l1_ratio=0.2, penalty=l2, solver=sag;, score=0.013 total time= 1.6min\n",
      "[CV 3/5; 29/50] START C=0.001, class_weight=None, l1_ratio=0.2, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 29/50] END C=0.001, class_weight=None, l1_ratio=0.2, penalty=l2, solver=sag;, score=0.013 total time= 1.7min\n",
      "[CV 4/5; 29/50] START C=0.001, class_weight=None, l1_ratio=0.2, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 29/50] END C=0.001, class_weight=None, l1_ratio=0.2, penalty=l2, solver=sag;, score=0.013 total time= 1.5min\n",
      "[CV 5/5; 29/50] START C=0.001, class_weight=None, l1_ratio=0.2, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 29/50] END C=0.001, class_weight=None, l1_ratio=0.2, penalty=l2, solver=sag;, score=0.013 total time= 1.6min\n",
      "[CV 1/5; 30/50] START C=0.05, class_weight=None, l1_ratio=0.6, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 30/50] END C=0.05, class_weight=None, l1_ratio=0.6, penalty=l2, solver=sag;, score=0.022 total time= 1.1min\n",
      "[CV 2/5; 30/50] START C=0.05, class_weight=None, l1_ratio=0.6, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 30/50] END C=0.05, class_weight=None, l1_ratio=0.6, penalty=l2, solver=sag;, score=0.023 total time=  59.9s\n",
      "[CV 3/5; 30/50] START C=0.05, class_weight=None, l1_ratio=0.6, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 30/50] END C=0.05, class_weight=None, l1_ratio=0.6, penalty=l2, solver=sag;, score=0.022 total time= 1.1min\n",
      "[CV 4/5; 30/50] START C=0.05, class_weight=None, l1_ratio=0.6, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 30/50] END C=0.05, class_weight=None, l1_ratio=0.6, penalty=l2, solver=sag;, score=0.022 total time=  54.6s\n",
      "[CV 5/5; 30/50] START C=0.05, class_weight=None, l1_ratio=0.6, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 30/50] END C=0.05, class_weight=None, l1_ratio=0.6, penalty=l2, solver=sag;, score=0.022 total time=  51.4s\n",
      "[CV 1/5; 31/50] START C=0.005, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 31/50] END C=0.005, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=sag;, score=0.064 total time=  57.3s\n",
      "[CV 2/5; 31/50] START C=0.005, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 31/50] END C=0.005, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=sag;, score=0.064 total time= 1.1min\n",
      "[CV 3/5; 31/50] START C=0.005, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 31/50] END C=0.005, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=sag;, score=0.064 total time= 1.1min\n",
      "[CV 4/5; 31/50] START C=0.005, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 31/50] END C=0.005, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=sag;, score=0.061 total time= 1.1min\n",
      "[CV 5/5; 31/50] START C=0.005, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 31/50] END C=0.005, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=sag;, score=0.063 total time=  50.3s\n",
      "[CV 1/5; 32/50] START C=0.5, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 32/50] END C=0.5, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg;, score=0.105 total time=18.6min\n",
      "[CV 2/5; 32/50] START C=0.5, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 32/50] END C=0.5, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg;, score=0.105 total time=16.5min\n",
      "[CV 3/5; 32/50] START C=0.5, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 32/50] END C=0.5, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg;, score=0.107 total time=17.9min\n",
      "[CV 4/5; 32/50] START C=0.5, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 32/50] END C=0.5, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg;, score=0.104 total time=12.6min\n",
      "[CV 5/5; 32/50] START C=0.5, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 32/50] END C=0.5, class_weight=balanced, l1_ratio=0.7, penalty=l2, solver=newton-cg;, score=0.106 total time=14.0min\n",
      "[CV 1/5; 33/50] START C=0.1, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=sag\n",
      "[CV 1/5; 33/50] END C=0.1, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/5; 33/50] START C=0.1, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=sag\n",
      "[CV 2/5; 33/50] END C=0.1, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/5; 33/50] START C=0.1, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=sag\n",
      "[CV 3/5; 33/50] END C=0.1, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/5; 33/50] START C=0.1, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=sag\n",
      "[CV 4/5; 33/50] END C=0.1, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/5; 33/50] START C=0.1, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=sag\n",
      "[CV 5/5; 33/50] END C=0.1, class_weight=None, l1_ratio=0.9, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/5; 34/50] START C=0.5, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga\n",
      "[CV 1/5; 34/50] END C=0.5, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga;, score=0.063 total time=17.6min\n",
      "[CV 2/5; 34/50] START C=0.5, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga\n",
      "[CV 2/5; 34/50] END C=0.5, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga;, score=0.063 total time=16.1min\n",
      "[CV 3/5; 34/50] START C=0.5, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga\n",
      "[CV 3/5; 34/50] END C=0.5, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga;, score=0.065 total time=16.9min\n",
      "[CV 4/5; 34/50] START C=0.5, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga\n",
      "[CV 4/5; 34/50] END C=0.5, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga;, score=0.061 total time=19.0min\n",
      "[CV 5/5; 34/50] START C=0.5, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga\n",
      "[CV 5/5; 34/50] END C=0.5, class_weight=None, l1_ratio=0.7, penalty=elasticnet, solver=saga;, score=0.060 total time=20.5min\n",
      "[CV 1/5; 35/50] START C=0.5, class_weight=None, l1_ratio=0.1, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 35/50] END C=0.5, class_weight=None, l1_ratio=0.1, penalty=l2, solver=saga;, score=0.049 total time= 1.9min\n",
      "[CV 2/5; 35/50] START C=0.5, class_weight=None, l1_ratio=0.1, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 35/50] END C=0.5, class_weight=None, l1_ratio=0.1, penalty=l2, solver=saga;, score=0.050 total time= 1.9min\n",
      "[CV 3/5; 35/50] START C=0.5, class_weight=None, l1_ratio=0.1, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 35/50] END C=0.5, class_weight=None, l1_ratio=0.1, penalty=l2, solver=saga;, score=0.051 total time= 2.0min\n",
      "[CV 4/5; 35/50] START C=0.5, class_weight=None, l1_ratio=0.1, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 35/50] END C=0.5, class_weight=None, l1_ratio=0.1, penalty=l2, solver=saga;, score=0.050 total time= 2.2min\n",
      "[CV 5/5; 35/50] START C=0.5, class_weight=None, l1_ratio=0.1, penalty=l2, solver=saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 35/50] END C=0.5, class_weight=None, l1_ratio=0.1, penalty=l2, solver=saga;, score=0.050 total time= 2.0min\n",
      "[CV 1/5; 36/50] START C=1.0, class_weight=balanced, l1_ratio=0.8, penalty=elasticnet, solver=saga\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tuned_log_reg = RandomizedSearchCV(LogisticRegression(), \n",
    "                                   param_list, \n",
    "                                   verbose = 10,\n",
    "                                   scoring = make_scorer(f1_score, **{\"average\" : \"macro\"}),\n",
    "                                   n_iter = 50, # only few iterations because n_jobs = -1 crashes the notebook\n",
    "                                   #n_jobs = -1   \n",
    "                                  ).fit(X_train_tfidf, train_labels[ : -100_000].argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904dde44-5176-428d-9adc-17f825d75f90",
   "metadata": {},
   "source": [
    "My hp tuning got interrupted by something inside jupyter, so it froze and failed to complete\n",
    "\n",
    "I'll pick the one that appears best and move on : `C=1.0, class_weight=balanced, l1_ratio=0.2, penalty=l2, solver=saga`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2dc06-8c4f-4aa0-bb3e-dd004740036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_tfidf_tuned = OneVsRestClassifier(\n",
    "    LogisticRegression(C = 1.0, \n",
    "                       class_weight = \"balanced\", \n",
    "                       #l1_ratio = 0.2, \n",
    "                       penalty = \"l2\", \n",
    "                       solver = \"saga\",\n",
    "                       max_iter = 500\n",
    "        )\n",
    "    ).fit(X_train_tfidf, train_labels[ : -100_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c910e9-a194-4dd1-8981-a1c301a9b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(log_reg_tfidf_tuned, \"tuned_clf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25479970-ccd7-46e5-be86-0d852f194278",
   "metadata": {},
   "source": [
    "### D. Clearly print Performance Metrics. [2 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e98ec-ce87-4826-bbfe-28cdb4eadd23",
   "metadata": {},
   "source": [
    "Performance on __training__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c63beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels[ : -100_000], tuned_log_reg_preds))\n",
    "print(f\"ROC_AUC score :\\t{roc_auc_score(test_labels[ : -100_000], tuned_log_reg_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44581cde-8ec3-435e-870e-fb8c105a432e",
   "metadata": {},
   "source": [
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.02      0.86      0.03      2404\n",
    "           1       0.01      0.92      0.02      3043\n",
    "           2       0.00      0.88      0.01       798\n",
    "           3       0.01      0.92      0.01      1061\n",
    "           4       0.11      0.75      0.19     21031\n",
    "           5       0.00      0.85      0.01       823\n",
    "           6       0.01      0.96      0.02      2565\n",
    "           7       0.01      0.85      0.02      1490\n",
    "           8       0.01      0.88      0.02      2901\n",
    "           9       0.01      0.93      0.02      2574\n",
    "          10       0.03      0.98      0.07     13062\n",
    "          11       0.07      0.39      0.12       677\n",
    "          12       0.01      0.98      0.02      3791\n",
    "          13       0.07      0.88      0.13     19212\n",
    "          14       0.47      0.18      0.26      7612\n",
    "          15       0.00      1.00      0.00       392\n",
    "          16       0.02      0.86      0.03      3160\n",
    "          17       0.29      0.28      0.28      4441\n",
    "          18       0.26      0.21      0.23      1931\n",
    "          19       0.24      0.32      0.27     10396\n",
    "          20       0.01      0.83      0.01       832\n",
    "          21       0.37      0.19      0.25      5844\n",
    "          22       0.03      0.60      0.05      1208\n",
    "          23       0.49      0.29      0.36      1507\n",
    "          24       0.75      0.32      0.45       179\n",
    "          25       0.02      0.76      0.04      3102\n",
    "          26       0.26      0.32      0.29      2037\n",
    "          27       0.01      0.87      0.02      1993\n",
    "          28       0.20      0.38      0.26      9543\n",
    "          29       0.03      0.88      0.05      5052\n",
    "          30       0.00      1.00      0.01      1863\n",
    "          31       0.02      0.84      0.05      3390\n",
    "          32       0.11      0.44      0.18      4748\n",
    "          33       0.00      1.00      0.01      1948\n",
    "          34       0.49      0.76      0.59     99802\n",
    "          35       0.32      0.46      0.38     27450\n",
    "          36       0.01      0.94      0.02      2529\n",
    "          37       1.00      0.00      0.00      1268\n",
    "          38       0.17      0.44      0.25      1411\n",
    "          39       0.59      0.71      0.64    162208\n",
    "\n",
    "   micro avg       0.05      0.69      0.10    441278\n",
    "   macro avg       0.16      0.67      0.14    441278\n",
    "weighted avg       0.39      0.69      0.44    441278\n",
    " samples avg       0.05      0.69      0.10    441278\n",
    "\n",
    "ROC_AUC score :\t0.6749733509951363\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97edbaa8-8021-48be-9796-9a0bb75ac0ab",
   "metadata": {},
   "source": [
    "Performance on __testing__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ae24cd-ce39-4913-9fb5-4dae893244ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_reg_tfidf_tuned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tuned_log_reg_preds \u001b[38;5;241m=\u001b[39m \u001b[43mlog_reg_tfidf_tuned\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test_tfidf)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(test_labels[ : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100_000\u001b[39m], tuned_log_reg_preds))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC_AUC score :\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mroc_auc_score(test_labels[ : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100_000\u001b[39m], tuned_log_reg_preds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'log_reg_tfidf_tuned' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'log_reg_tfidf_tuned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tuned_log_reg_preds \u001b[38;5;241m=\u001b[39m \u001b[43mlog_reg_tfidf_tuned\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test_tfidf)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(test_labels[ : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100_000\u001b[39m], tuned_log_reg_preds))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC_AUC score :\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mroc_auc_score(test_labels[ : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100_000\u001b[39m], tuned_log_reg_preds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'log_reg_tfidf_tuned' is not defined"
     ]
    }
   ],
   "source": [
    "tuned_log_reg_preds = log_reg_tfidf_tuned.predict(X_test_tfidf)\n",
    "print(classification_report(test_labels[ : -100_000], tuned_log_reg_preds))\n",
    "print(f\"ROC_AUC score :\\t{roc_auc_score(test_labels[ : -100_000], tuned_log_reg_preds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d210c-7a61-4738-a4e7-7e202679a765",
   "metadata": {},
   "source": [
    "### 5. Share insights on relative performance comparison [8 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a10720-dfa7-4e6d-bb77-de665fef04dd",
   "metadata": {},
   "source": [
    "#### A. Which vectorizer performed better? Probable reason?."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c08e2a-d7d9-4951-8239-65b10dc4b974",
   "metadata": {},
   "source": [
    "- The original model performed poorly, even though I used a much more robust vectorizer like `glove`, and even the entire training dataset via libraries like `hdf5`\n",
    "- It's possible the performance could've improved by hyperparameter tuning, but that is infeasible for me, it took 6-8 hours just to train that model, and tuned parameters trained on a smaller dataset don't translate properly to the larger dataset either\n",
    "- The data was then vectorized with `Bow (1, 2)-grams` and `tf_idf 1-gram` vectorizers. neither vectorizer improved performance by themselves, although a relatively large subset of the data could now be stored in memory by `tf_idf 1-gram`.\n",
    "- Overall `tf_idf 1-gram` became the preferred vectorizer by virtue of not crashing my machine when I used it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccbd2f-8624-423e-810b-eabab31ecc1a",
   "metadata": {},
   "source": [
    "####  B. Which model outperformed? Probable reason? [2 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30902cc-e0fb-46d3-8e89-0d89de342c4e",
   "metadata": {},
   "source": [
    "- Tuned `SGD Logistic Regression` trained on subset of data outperformed `SGD linear SVC` trained on entire data\n",
    "- I don't know if one model is strictly better than the other sicne they both trained on different representations of the data, and the cost of computing on the `glove` vectorized dataset is too expensive for me to try again\n",
    "- However, the __reason__ why LR outdid SVM seems to be because of better data vectorization in `LR` since I'm seeing othe people's code where using `glove` and/or `word2vec` hurt performance for them as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ec2c4-660a-41b4-88d6-abb3c3ef1e9d",
   "metadata": {},
   "source": [
    "### C. Which parameter/hyperparameter significantly helped to improve performance?Probable reason?. [2 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b5950-b1a3-4668-8711-76447c3792d5",
   "metadata": {},
   "source": [
    "- The `loss` hyperparameter seems among the most significant, since it actually seems to take the `SGD` nature of the classifier into account, unlike the others\n",
    "- The second would be `class_weights`, important since the dataset is very imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7187b-b064-4fc8-9891-eaa18acb8ea9",
   "metadata": {},
   "source": [
    "### D. According to you, which performance metric should be given most importance, why?. [2 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65bf716-1f4a-4f00-8e51-711f7b854fff",
   "metadata": {},
   "source": [
    "- `f1-score`, since we need to somehow account for the fact that the data is very imbalanced, resulting in a dumb classifier easily getting 30%+ accuracy by simply classifying every point as `39` in the test dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720e13f-40b4-469c-bf2a-90073a2f9c55",
   "metadata": {},
   "source": [
    "# Part B - 20 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4bdb8-54e8-4a9c-8a09-e7b95c382013",
   "metadata": {},
   "source": [
    "__DOMAIN__: Customer support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2360b-24fb-4ee7-a433-297e3d54ceba",
   "metadata": {},
   "source": [
    "__CONTEXT__: Great Learning has a an academic support department which receives numerous support requests every day throughout the year.Teams are spread across geographies and try to provide support round the year. Sometimes there are circumstances where due to heavy workload certain request resolutions are delayed, impacting company’s business. Some of the requests are very generic where a proper resolution procedure delivered to the user can solve the problem. Company is looking forward to design an automation which can interact with the user, understand the problem and display the resolution procedure (if found as a generic request) or redirect the request to an actual human support executive if the request is complex or not in it’s database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1d7b08-ab6f-4091-aefe-693467ee3f8b",
   "metadata": {},
   "source": [
    "__DATA DESCRIPTION__: A sample corpus is attached for your reference. Please enhance/add more data to the corpus using your linguistics skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9bcf83-2eeb-4b79-b7e3-d9157eeebc2e",
   "metadata": {},
   "source": [
    "__PROJECT OBJECTIVE__: Design a python based interactive semi - rule based chatbot which can do the following:\n",
    "1. Start chat session with greetings and ask what the user is looking for. [5 Marks]\n",
    "2. Accept dynamic text based questions from the user. Reply back with relevant answer from the designed corpus. [10 Marks]\n",
    "3. End the chat session only if the user requests to end else ask what the user is looking for. Loop continues till the user asks to end it. [5 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0622da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.0/en_core_web_lg-3.4.0-py3-none-any.whl (587.7 MB)\n",
      "     -------------------------------------- 587.7/587.7 MB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from en-core-web-lg==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.23.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.21.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (49.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.24.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.4.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 21:46:22.618957: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-09-07 21:46:22.618998: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-09-07 21:46:26.156122: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-09-07 21:46:26.156148: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-07 21:46:26.162703: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: Sam\n",
      "2022-09-07 21:46:26.162890: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: Sam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.0/en_core_web_lg-3.4.0-py3-none-any.whl (587.7 MB)\n",
      "     -------------------------------------- 587.7/587.7 MB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from en-core-web-lg==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.23.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.21.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (49.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.24.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.4.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 21:46:22.618957: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-09-07 21:46:22.618998: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-09-07 21:46:26.156122: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-09-07 21:46:26.156148: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-07 21:46:26.162703: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: Sam\n",
      "2022-09-07 21:46:26.162890: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: Sam\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb747cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.4.1-cp38-cp38-win_amd64.whl (12.1 MB)\n",
      "     ---------------------------------------- 12.1/12.1 MB 1.5 MB/s eta 0:00:00\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ------------------------------------ 181.6/181.6 kB 844.4 kB/s eta 0:00:00\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy) (1.23.2)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.7-cp38-cp38-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.7/96.7 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.8/42.8 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.8-cp38-cp38-win_amd64.whl (18 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy) (49.2.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4\n",
      "  Downloading pydantic-1.9.2-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 2.6 MB/s eta 0:00:00\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.0-cp38-cp38-win_amd64.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.4-cp38-cp38-win_amd64.whl (449 kB)\n",
      "     -------------------------------------- 449.9/449.9 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.6/58.6 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.8-cp38-cp38-win_amd64.whl (6.6 MB)\n",
      "     ---------------------------------------- 6.6/6.6 MB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: wasabi, cymem, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, typer, srsly, preshed, thinc, pathy, spacy\n",
      "Successfully installed blis-0.7.8 catalogue-2.0.8 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.8 pathy-0.6.2 preshed-3.0.7 pydantic-1.9.2 smart-open-5.2.1 spacy-3.4.1 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.4 thinc-8.1.0 typer-0.4.2 wasabi-0.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000173FBB9DE50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000173FBB9DFA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000173FBBC1340>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000173FBBC14F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/spacy/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.4.1-cp38-cp38-win_amd64.whl (12.1 MB)\n",
      "     ---------------------------------------- 12.1/12.1 MB 1.5 MB/s eta 0:00:00\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ------------------------------------ 181.6/181.6 kB 844.4 kB/s eta 0:00:00\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy) (1.23.2)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.7-cp38-cp38-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.7/96.7 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.8/42.8 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.8-cp38-cp38-win_amd64.whl (18 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy) (49.2.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4\n",
      "  Downloading pydantic-1.9.2-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 2.6 MB/s eta 0:00:00\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.0-cp38-cp38-win_amd64.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.4-cp38-cp38-win_amd64.whl (449 kB)\n",
      "     -------------------------------------- 449.9/449.9 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.6/58.6 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.8-cp38-cp38-win_amd64.whl (6.6 MB)\n",
      "     ---------------------------------------- 6.6/6.6 MB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: wasabi, cymem, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, typer, srsly, preshed, thinc, pathy, spacy\n",
      "Successfully installed blis-0.7.8 catalogue-2.0.8 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.8 pathy-0.6.2 preshed-3.0.7 pydantic-1.9.2 smart-open-5.2.1 spacy-3.4.1 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.4 thinc-8.1.0 typer-0.4.2 wasabi-0.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000173FBB9DE50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000173FBB9DFA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000173FBBC1340>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000173FBBC14F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/spacy/\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f0408d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (22.2.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (49.2.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.37.1)\n",
      "Requirement already satisfied: pip in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (22.2.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (49.2.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\sam\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.37.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/wheel/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/wheel/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/wheel/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/wheel/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/wheel/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/wheel/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/wheel/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/wheel/\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e091e6b4-4273-42b0-8fa4-34bcd23503f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f9c373d-7617-4394-be8e-3b4eee9fc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_agent = spacy.load('en_core_web_lg')\n",
    "with open(\"GL Bot.json\", \"r\") as f:\n",
    "    corpus = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46c18378-9083-4093-95f9-29f28e8a33cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tag', 'patterns', 'responses', 'context_set'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['tag', 'patterns', 'responses', 'context_set'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[\"intents\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c0634f-abb3-4f95-8508-87f7846a769a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Intro'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Intro'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[\"intents\"][0][\"tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ba93362-e4ba-4405-a870-b660dce845cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [intent[\"tag\"] for intent in corpus[\"intents\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b43d76c6-bef6-4a34-9a20-032986ce2cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Intro', 'Exit', 'Olympus', 'SL', 'NN', 'Bot', 'Profane', 'Ticket']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Intro', 'Exit', 'Olympus', 'SL', 'NN', 'Bot', 'Profane', 'Ticket']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6e80d9f-57bc-400b-907c-5964a02e5de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {}\n",
    "for intent,tag in zip(corpus[\"intents\"], tags):\n",
    "    patterns[tag] = intent[\"patterns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5c0a0f8-1aad-4f80-8dbe-dbf8f4cc8e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Intro', 'Exit', 'Olympus', 'SL', 'NN', 'Bot', 'Profane', 'Ticket'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['Intro', 'Exit', 'Olympus', 'SL', 'NN', 'Bot', 'Profane', 'Ticket'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30e931b0-52ee-4415-bb75-744a2a0b57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = {}\n",
    "for intent,tag in zip(corpus[\"intents\"], tags):\n",
    "    responses[tag] = intent[\"responses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "715d8739-5629-4d14-98b5-aae1027533cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Intro', 'Exit', 'Olympus', 'SL', 'NN', 'Bot', 'Profane', 'Ticket'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['Intro', 'Exit', 'Olympus', 'SL', 'NN', 'Bot', 'Profane', 'Ticket'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "195ac104-8a24-47d9-ac89-2bf07946e565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlu_agent(\"hello\").similarity(nlu_agent(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3101c-e01f-47af-bf84-c758dddf96ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hope I was able to encourage you, Good Bye'\n",
      "max_similarity = 0.11733216115001627 for user_intent = 'Intro'\n",
      "max_similarity = 0.6205560258085092 for user_intent = 'Intro'\n",
      "max_similarity = 0.657622820326334 for user_intent = 'Intro'\n",
      "max_similarity = 0.7559291230031941 for user_intent = 'Intro'\n",
      "max_similarity = 0.761843886005391 for user_intent = 'Intro'\n",
      "max_similarity = 0.11733216115001627 for user_intent = 'Intro'\n",
      "max_similarity = 0.6205560258085092 for user_intent = 'Intro'\n",
      "max_similarity = 0.657622820326334 for user_intent = 'Intro'\n",
      "max_similarity = 0.7559291230031941 for user_intent = 'Intro'\n",
      "max_similarity = 0.761843886005391 for user_intent = 'Intro'\n",
      "max_similarity = 0.8502959569859891 for user_intent = 'Olympus'\n",
      "max_similarity = 0.8502959569859891 for user_intent = 'Olympus'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_26812\\2187551628.py:7: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similarity = nlu_agent(text).similarity(nlu_agent(pattern))\n",
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_26812\\2187551628.py:7: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similarity = nlu_agent(text).similarity(nlu_agent(pattern))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Link: Olympus wiki']\n",
      "['Link: Olympus wiki']\n",
      "Hello! may i help you\n",
      "max_similarity = 0.2315078385729302 for user_intent = 'Intro'\n",
      "max_similarity = 0.6678389371005324 for user_intent = 'Intro'\n",
      "max_similarity = 0.7339450005774003 for user_intent = 'Intro'\n",
      "max_similarity = 0.7677422844319655 for user_intent = 'Intro'\n",
      "max_similarity = 0.2315078385729302 for user_intent = 'Intro'\n",
      "max_similarity = 0.6678389371005324 for user_intent = 'Intro'\n",
      "max_similarity = 0.7339450005774003 for user_intent = 'Intro'\n",
      "max_similarity = 0.7677422844319655 for user_intent = 'Intro'\n",
      "max_similarity = 0.7915181915621307 for user_intent = 'SL'\n",
      "max_similarity = 0.7944862547168513 for user_intent = 'SL'\n",
      "max_similarity = 0.7915181915621307 for user_intent = 'SL'\n",
      "max_similarity = 0.7944862547168513 for user_intent = 'SL'\n",
      "max_similarity = 0.8417538893531247 for user_intent = 'Profane'\n",
      "['Please use respectful words']\n",
      "max_similarity = 0.8417538893531247 for user_intent = 'Profane'\n",
      "['Please use respectful words']\n",
      "i can understand that i was unable to do that\n",
      "max_similarity = 0.11581570364350842 for user_intent = 'Intro'\n",
      "max_similarity = 0.6823973112205786 for user_intent = 'Intro'\n",
      "max_similarity = 0.8936138840310701 for user_intent = 'Intro'\n",
      "max_similarity = 0.11581570364350842 for user_intent = 'Intro'\n",
      "max_similarity = 0.6823973112205786 for user_intent = 'Intro'\n",
      "max_similarity = 0.8936138840310701 for user_intent = 'Intro'\n",
      "max_similarity = 0.8970374205516803 for user_intent = 'SL'\n",
      "max_similarity = 0.9028081437665464 for user_intent = 'SL'\n",
      "max_similarity = 0.9029358447398528 for user_intent = 'SL'\n",
      "max_similarity = 0.8970374205516803 for user_intent = 'SL'\n",
      "max_similarity = 0.9028081437665464 for user_intent = 'SL'\n",
      "max_similarity = 0.9029358447398528 for user_intent = 'SL'\n",
      "['Link: Machine Learning wiki ']\n",
      "['Link: Machine Learning wiki ']\n",
      "i can do it as much i can do better\n",
      "max_similarity = 0.14960275401398104 for user_intent = 'Intro'\n",
      "max_similarity = 0.6863369985219028 for user_intent = 'Intro'\n",
      "max_similarity = 0.7476971478481219 for user_intent = 'Intro'\n",
      "max_similarity = 0.14960275401398104 for user_intent = 'Intro'\n",
      "max_similarity = 0.6863369985219028 for user_intent = 'Intro'\n",
      "max_similarity = 0.7476971478481219 for user_intent = 'Intro'\n",
      "max_similarity = 0.7505843670871902 for user_intent = 'Olympus'\n",
      "max_similarity = 0.7879169097720281 for user_intent = 'SL'\n",
      "max_similarity = 0.7938941570107887 for user_intent = 'SL'\n",
      "max_similarity = 0.7505843670871902 for user_intent = 'Olympus'\n",
      "max_similarity = 0.7879169097720281 for user_intent = 'SL'\n",
      "max_similarity = 0.7938941570107887 for user_intent = 'SL'\n",
      "max_similarity = 0.7983329894077356 for user_intent = 'Profane'\n",
      "['Please use respectful words']\n",
      "max_similarity = 0.7983329894077356 for user_intent = 'Profane'\n",
      "['Please use respectful words']\n",
      "i hope i done it very well, good bye\n",
      "max_similarity = 0.23910802970243408 for user_intent = 'Intro'\n",
      "max_similarity = 0.5785153180742673 for user_intent = 'Intro'\n",
      "max_similarity = 0.6930092062294602 for user_intent = 'Intro'\n",
      "max_similarity = 0.712317202823697 for user_intent = 'Intro'\n",
      "max_similarity = 0.23910802970243408 for user_intent = 'Intro'\n",
      "max_similarity = 0.5785153180742673 for user_intent = 'Intro'\n",
      "max_similarity = 0.6930092062294602 for user_intent = 'Intro'\n",
      "max_similarity = 0.712317202823697 for user_intent = 'Intro'\n",
      "max_similarity = 0.7528138368594485 for user_intent = 'SL'\n",
      "max_similarity = 0.7589676520459997 for user_intent = 'SL'\n",
      "max_similarity = 0.7596458690033043 for user_intent = 'SL'\n",
      "max_similarity = 0.7636493296896208 for user_intent = 'SL'\n",
      "max_similarity = 0.7528138368594485 for user_intent = 'SL'\n",
      "max_similarity = 0.7589676520459997 for user_intent = 'SL'\n",
      "max_similarity = 0.7596458690033043 for user_intent = 'SL'\n",
      "max_similarity = 0.7636493296896208 for user_intent = 'SL'\n",
      "max_similarity = 0.8301275882435183 for user_intent = 'Profane'\n",
      "['Please use respectful words']\n",
      "max_similarity = 0.8301275882435183 for user_intent = 'Profane'\n",
      "['Please use respectful words']\n",
      "nothing is impossible\n",
      "max_similarity = 0.5048150476370683 for user_intent = 'Intro'\n",
      "max_similarity = 0.9355536022730674 for user_intent = 'Intro'\n",
      "max_similarity = 0.5048150476370683 for user_intent = 'Intro'\n",
      "max_similarity = 0.9355536022730674 for user_intent = 'Intro'\n",
      "['Hello! how can i help you ?']\n",
      "['Hello! how can i help you ?']\n",
      "Hello! how can i help you ?\n",
      "max_similarity = 0.1972901253455319 for user_intent = 'Intro'\n",
      "max_similarity = 0.7594149546519735 for user_intent = 'Intro'\n",
      "max_similarity = 0.7663639079941896 for user_intent = 'Intro'\n",
      "max_similarity = 0.1972901253455319 for user_intent = 'Intro'\n",
      "max_similarity = 0.7594149546519735 for user_intent = 'Intro'\n",
      "max_similarity = 0.7663639079941896 for user_intent = 'Intro'\n",
      "max_similarity = 0.7817666960660666 for user_intent = 'Profane'\n",
      "max_similarity = 0.8052950368903746 for user_intent = 'Profane'\n",
      "max_similarity = 0.8172304918779125 for user_intent = 'Ticket'\n",
      "['Tarnsferring the request to your PM']\n",
      "max_similarity = 0.7817666960660666 for user_intent = 'Profane'\n",
      "max_similarity = 0.8052950368903746 for user_intent = 'Profane'\n",
      "max_similarity = 0.8172304918779125 for user_intent = 'Ticket'\n",
      "['Tarnsferring the request to your PM']\n",
      "'Tarnsferring the request to your PM'\n",
      "max_similarity = 0.3148008747716285 for user_intent = 'Intro'\n",
      "max_similarity = 0.3414036892774837 for user_intent = 'Intro'\n",
      "max_similarity = 0.4393980364892121 for user_intent = 'Intro'\n",
      "max_similarity = 0.5683931842122228 for user_intent = 'Intro'\n",
      "max_similarity = 0.3148008747716285 for user_intent = 'Intro'\n",
      "max_similarity = 0.3414036892774837 for user_intent = 'Intro'\n",
      "max_similarity = 0.4393980364892121 for user_intent = 'Intro'\n",
      "max_similarity = 0.5683931842122228 for user_intent = 'Intro'\n",
      "max_similarity = 0.5704859010672113 for user_intent = 'Olympus'\n",
      "max_similarity = 0.5704859010672113 for user_intent = 'Olympus'\n",
      "max_similarity = 0.5799926179787851 for user_intent = 'NN'\n",
      "max_similarity = 0.5799926179787851 for user_intent = 'NN'\n",
      "['Link: Neural Nets wiki']\n",
      "['Link: Neural Nets wiki']\n",
      "'Link: Machine Learning wiki '\n",
      "max_similarity = 0.012594346080713909 for user_intent = 'Intro'\n",
      "max_similarity = 0.02520149993904402 for user_intent = 'Intro'\n",
      "max_similarity = 0.10057895336060405 for user_intent = 'Intro'\n",
      "max_similarity = 0.13580625552532555 for user_intent = 'Intro'\n",
      "max_similarity = 0.012594346080713909 for user_intent = 'Intro'\n",
      "max_similarity = 0.02520149993904402 for user_intent = 'Intro'\n",
      "max_similarity = 0.10057895336060405 for user_intent = 'Intro'\n",
      "max_similarity = 0.13580625552532555 for user_intent = 'Intro'\n",
      "max_similarity = 0.2012569127544915 for user_intent = 'SL'\n",
      "max_similarity = 0.24271469522583286 for user_intent = 'SL'\n",
      "max_similarity = 0.31080700990217147 for user_intent = 'SL'\n",
      "max_similarity = 0.2012569127544915 for user_intent = 'SL'\n",
      "max_similarity = 0.24271469522583286 for user_intent = 'SL'\n",
      "max_similarity = 0.31080700990217147 for user_intent = 'SL'\n",
      "['Link: Machine Learning wiki ']\n",
      "['Link: Machine Learning wiki ']\n",
      "I hope I was able to assist you, Good Bye\n",
      "max_similarity = 0.10681052244203872 for user_intent = 'Intro'\n",
      "max_similarity = 0.6410017325877474 for user_intent = 'Intro'\n",
      "max_similarity = 0.6859939607637126 for user_intent = 'Intro'\n",
      "max_similarity = 0.7670239459856357 for user_intent = 'Intro'\n",
      "max_similarity = 0.7807256686241136 for user_intent = 'Intro'\n",
      "max_similarity = 0.10681052244203872 for user_intent = 'Intro'\n",
      "max_similarity = 0.6410017325877474 for user_intent = 'Intro'\n",
      "max_similarity = 0.6859939607637126 for user_intent = 'Intro'\n",
      "max_similarity = 0.7670239459856357 for user_intent = 'Intro'\n",
      "max_similarity = 0.7807256686241136 for user_intent = 'Intro'\n",
      "max_similarity = 0.8833719012997977 for user_intent = 'Olympus'\n",
      "max_similarity = 0.8833719012997977 for user_intent = 'Olympus'\n",
      "['Link: Olympus wiki']\n",
      "['Link: Olympus wiki']\n",
      "Link: Machine Learning wiki\n",
      "max_similarity = 0.0020196652286047848 for user_intent = 'Intro'\n",
      "max_similarity = 0.04428788492207187 for user_intent = 'Intro'\n",
      "max_similarity = 0.1390546493272673 for user_intent = 'Intro'\n",
      "max_similarity = 0.2918920567944422 for user_intent = 'Intro'\n",
      "max_similarity = 0.0020196652286047848 for user_intent = 'Intro'\n",
      "max_similarity = 0.04428788492207187 for user_intent = 'Intro'\n",
      "max_similarity = 0.1390546493272673 for user_intent = 'Intro'\n",
      "max_similarity = 0.2918920567944422 for user_intent = 'Intro'\n",
      "max_similarity = 0.3201970164544284 for user_intent = 'SL'\n",
      "max_similarity = 0.3630013263355487 for user_intent = 'SL'\n",
      "max_similarity = 0.40120008647908 for user_intent = 'SL'\n",
      "max_similarity = 0.3201970164544284 for user_intent = 'SL'\n",
      "max_similarity = 0.3630013263355487 for user_intent = 'SL'\n",
      "max_similarity = 0.40120008647908 for user_intent = 'SL'\n",
      "['Link: Machine Learning wiki ']\n",
      "['Link: Machine Learning wiki ']\n",
      "Link: Machine Learning wiki\n",
      "max_similarity = 0.0020196652286047848 for user_intent = 'Intro'\n",
      "max_similarity = 0.04428788492207187 for user_intent = 'Intro'\n",
      "max_similarity = 0.1390546493272673 for user_intent = 'Intro'\n",
      "max_similarity = 0.2918920567944422 for user_intent = 'Intro'\n",
      "max_similarity = 0.0020196652286047848 for user_intent = 'Intro'\n",
      "max_similarity = 0.04428788492207187 for user_intent = 'Intro'\n",
      "max_similarity = 0.1390546493272673 for user_intent = 'Intro'\n",
      "max_similarity = 0.2918920567944422 for user_intent = 'Intro'\n",
      "max_similarity = 0.3201970164544284 for user_intent = 'SL'\n",
      "max_similarity = 0.3630013263355487 for user_intent = 'SL'\n",
      "max_similarity = 0.40120008647908 for user_intent = 'SL'\n",
      "max_similarity = 0.3201970164544284 for user_intent = 'SL'\n",
      "max_similarity = 0.3630013263355487 for user_intent = 'SL'\n",
      "max_similarity = 0.40120008647908 for user_intent = 'SL'\n",
      "['Link: Machine Learning wiki ']\n",
      "['Link: Machine Learning wiki ']\n",
      "I hope I was able to assist you, Good Bye\n",
      "max_similarity = 0.10681052244203872 for user_intent = 'Intro'\n",
      "max_similarity = 0.6410017325877474 for user_intent = 'Intro'\n",
      "max_similarity = 0.6859939607637126 for user_intent = 'Intro'\n",
      "max_similarity = 0.7670239459856357 for user_intent = 'Intro'\n",
      "max_similarity = 0.7807256686241136 for user_intent = 'Intro'\n",
      "max_similarity = 0.10681052244203872 for user_intent = 'Intro'\n",
      "max_similarity = 0.6410017325877474 for user_intent = 'Intro'\n",
      "max_similarity = 0.6859939607637126 for user_intent = 'Intro'\n",
      "max_similarity = 0.7670239459856357 for user_intent = 'Intro'\n",
      "max_similarity = 0.7807256686241136 for user_intent = 'Intro'\n",
      "max_similarity = 0.8833719012997977 for user_intent = 'Olympus'\n",
      "max_similarity = 0.8833719012997977 for user_intent = 'Olympus'\n",
      "['Link: Olympus wiki']\n",
      "['Link: Olympus wiki']\n",
      "Link: Olympus wiki\n",
      "max_similarity = 0.007595331127708853 for user_intent = 'Intro'\n",
      "max_similarity = 0.02244189573250018 for user_intent = 'Intro'\n",
      "max_similarity = 0.19504085371505803 for user_intent = 'Intro'\n",
      "max_similarity = 0.007595331127708853 for user_intent = 'Intro'\n",
      "max_similarity = 0.02244189573250018 for user_intent = 'Intro'\n",
      "max_similarity = 0.19504085371505803 for user_intent = 'Intro'\n",
      "max_similarity = 0.23755357804594895 for user_intent = 'Olympus'\n",
      "max_similarity = 0.275244482498209 for user_intent = 'SL'\n",
      "max_similarity = 0.23755357804594895 for user_intent = 'Olympus'\n",
      "max_similarity = 0.275244482498209 for user_intent = 'SL'\n",
      "max_similarity = 0.2942978223722346 for user_intent = 'SL'\n",
      "max_similarity = 0.2942978223722346 for user_intent = 'SL'\n",
      "['Link: Machine Learning wiki ']\n",
      "['Link: Machine Learning wiki ']\n",
      "Link: Machine Learning wiki\n",
      "max_similarity = 0.0020196652286047848 for user_intent = 'Intro'\n",
      "max_similarity = 0.04428788492207187 for user_intent = 'Intro'\n",
      "max_similarity = 0.1390546493272673 for user_intent = 'Intro'\n",
      "max_similarity = 0.2918920567944422 for user_intent = 'Intro'\n",
      "max_similarity = 0.0020196652286047848 for user_intent = 'Intro'\n",
      "max_similarity = 0.04428788492207187 for user_intent = 'Intro'\n",
      "max_similarity = 0.1390546493272673 for user_intent = 'Intro'\n",
      "max_similarity = 0.2918920567944422 for user_intent = 'Intro'\n",
      "max_similarity = 0.3201970164544284 for user_intent = 'SL'\n",
      "max_similarity = 0.3630013263355487 for user_intent = 'SL'\n",
      "max_similarity = 0.40120008647908 for user_intent = 'SL'\n",
      "max_similarity = 0.3201970164544284 for user_intent = 'SL'\n",
      "max_similarity = 0.3630013263355487 for user_intent = 'SL'\n",
      "max_similarity = 0.40120008647908 for user_intent = 'SL'\n",
      "['Link: Machine Learning wiki ']\n",
      "['Link: Machine Learning wiki ']\n",
      "I hope I was able to assist you, Good Bye\n",
      "max_similarity = 0.10681052244203872 for user_intent = 'Intro'\n",
      "max_similarity = 0.6410017325877474 for user_intent = 'Intro'\n",
      "max_similarity = 0.6859939607637126 for user_intent = 'Intro'\n",
      "max_similarity = 0.7670239459856357 for user_intent = 'Intro'\n",
      "max_similarity = 0.7807256686241136 for user_intent = 'Intro'\n",
      "max_similarity = 0.10681052244203872 for user_intent = 'Intro'\n",
      "max_similarity = 0.6410017325877474 for user_intent = 'Intro'\n",
      "max_similarity = 0.6859939607637126 for user_intent = 'Intro'\n",
      "max_similarity = 0.7670239459856357 for user_intent = 'Intro'\n",
      "max_similarity = 0.7807256686241136 for user_intent = 'Intro'\n",
      "max_similarity = 0.8833719012997977 for user_intent = 'Olympus'\n",
      "max_similarity = 0.8833719012997977 for user_intent = 'Olympus'\n",
      "['Link: Olympus wiki']\n",
      "['Link: Olympus wiki']\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    text = input()\n",
    "    user_intent = \"\"\n",
    "    max_similarity = 0\n",
    "    for intent, patterns_ in patterns.items():\n",
    "        for pattern in patterns_:\n",
    "            similarity = nlu_agent(text).similarity(nlu_agent(pattern))\n",
    "            if similarity > max_similarity:\n",
    "                user_intent = intent\n",
    "                max_similarity = similarity\n",
    "                print(f\"{max_similarity = } for {user_intent = }\")\n",
    "    print(responses[user_intent])\n",
    "    if user_intent == \"Exit\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e603e8-024a-4ee0-a8f3-0f6b1bb985a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
